\section{3041F: Time Series II Intro}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=41fa68b6-33c6-479e-b4a6-9909d6ec7edb}{Time Series Video 8}}
We'll be covering:
\begin{itemize}
    \item Stationarity, strict and weak variants.
    \item Autocorrelation: Measure the same variable but at different time points, and calculate the correlation between those timepoints
    \item Autocovariance
    \item Linear Time series models (moving average, autoregressive process, autoregressive moving average process)
\end{itemize}  

\subsection{Backshift Operator}
We'll (eventually) need the \textbf{Backshift Operator}, so define it as:
\begin{enumerate}
    \item \begin{equation*}
            B^iy_t = y_{t-i}
        \end{equation*}
    \item \begin{equation*}
            Bc = c
        \end{equation*}
    \item \begin{equation*}
            (B^i + B^j)y_t = B^iy_t + B^jy_t = y_{t-i} + y_{t-j}
        \end{equation*}
    \item \begin{equation*}
            B^iB^jy_t = B^jB^iy_t = B^iy_{t-j}= y_{t-j-i}   
        \end{equation*}
    \item Also, specially define the following:\begin{equation*}
            \theta(B) = 1 - \theta_1B - \theta_2B^2 - \dots - \theta_qB^q
    \end{equation*}
    \item For $|a| < 1$: \begin{equation*}
            (1 + aB + a^2B^2 + \dots )y_t = \frac{y_t}{1 - aB}
        \end{equation*}
    \item For $|a| > 1$: \begin{equation*}
            (1 + \frac{1}{aB} + \frac{1}{a^2B^2} + \dots )y_t = \frac{-aBy_t}{1
            - aB}
        \end{equation*}
    
\end{enumerate}
\section{Stationarity}
\subsection{Strict Stationarity}
Strict stationarity is a property of a time series.
\begin{itemize}
    \item A time series is strictly stationary iff for every offset parameter
        $m > 0$, all time points $t_1, \dots, t_n$, the joint distribution of
        $Z_{t_1}, \dots, Z_{t_n}$ is equal to that of $Z_{t_1 - m},
        \dots, Z_{t_n-m}$.
    \item We also require that if there are two points, $Z_t$ and  $Z_{t+m}$
        then  the distribution of the random vector  $\left( Z_t,
        Z_{t+m}\right)$ must be a function of the \textbf{lag} $|m|$.
    \item Given a timeline of a time series and an offset parameter $m$ 
    \item Consider Random Variables $Z_{t_1}, \dots, Z_{t_n}$, find the joint
        distribution of them all, then apply a timeshift by the offset
        parameter $m$: $Z_{t_{1-m}}, \dots, Z_{t_{n-m}}$.
    \item then if the joint distribution of the first set of Random variables
        is equal to that the joint distribution of the second set of random
        variables, then the time series is said to be \textbf{Strictly
        Stationary}.
\end{itemize}

\subsection{Autocovariance}
Define the \textbf{Autocovariance} as the covariance of the same random
variable but at different time points:
\begin{equation*}
    \begin{aligned}
        \mathbb{C}ov[Z_t, Z_{t+m}] &:= \gamma_{|m|} \\ 
                &= \mathbb{E}[Z_t \cdot Z_{t+m}] - \mathbb{E}[Z_t] \cdot
                \mathbb{E}[Z_{t+m}] \\
        &\text{Also note that:} \\
        \gamma_m = \gamma_{-m}
        \mathbb{V}[Z_t] &= \gamma_0 \\
    \end{aligned}
\end{equation*}
We can use $|m|$ because Covariance doesn't depend on the order of
the random variables given to it. 

Define the \textbf{Autocorrelation} as the autocovariance divided by the
variance of a time series. The autocorrelation (given the offset parameter
$m$) $\rho_m$ is defined by:
\begin{equation*}
    \begin{aligned}
        \rho_m &= \frac{\gamma_m}{\gamma_0} \\
        \rho_0 &= \frac{\gamma_0}{\gamma_0} = 1 \\
    \end{aligned}
\end{equation*}


\subsection{Weak Stationarity}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=c9dbfa7f-7ba8-4145-8037-26bdd235fd17}{Time Series Video 9}}

Weak stationarity is defined by orders (order 1, order 2, etc). So to have weak
stationarity of order $n$, the time series must have equal $n$-th moments.
For example, weak stationarity of order 1 requires a constant expectation.

Weak Stationarity of order 2 requires 
\begin{itemize}
    \item A constant expectation.
    \begin{equation*}
        \mathbb{E}[Z_t] = \mathbb{E}[Z_{t-m}]  \forall m
    \end{equation*}
        
    \item the autocovariance to only depend on the offset parameter $m$. For
        example, where $t$ and $s$ are valid indices to the time series $Z$:
    \begin{equation*}
        \mathbb{C}ov[Z_t, Z_{t-m}] = \mathbb{C}ov[Z_s, Z_{s-m}] 
    \end{equation*}
\end{itemize}
Therefore Weak Stationary of order 2 implies Weak Stationarity of order 1.

\subsection{Some examples of Weak stationarity}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=364aeb35-5750-4eb1-b844-93a239f8f81a}{Time Series Video 10}}
If we define a time series as $y_t = e_t - \frac{1}{2}e_{t-1}$ with all error
terms independant and $e_t \sim N(0, \sigma^2)$ then:

\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \mathbb{E}[e_t - \frac{1}{2}e_{t-1}] = 0 + 0 = 0 \\
        \gamma_0 &= \mathbb{E}[\left( e_t - \frac{1}{2}e_{t-1}\right)] - \mathbb{E}[y_t] \mathbb{E}[y_t]  \\
                 &= \dots \\
                 &= \sigma^2(1 + 0.25) + 0 \cdot 0 \\
                 &= 1.25 \sigma^2 \\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] \\
                 &= \mathbb{E}[\left( y_t \right) \left( y_{t-1}\right)] -
                 0 \cdot 0\\
                 &= \dots \\
                 &= -0.5\sigma^2 \\
        \gamma_k &= \mathbb{E}[y_t \cdot y_{t-k}]  - 0 \cdot 0\\
                 &= \mathbb{E}[\left( e_t - \frac{1}{2}e_{t-1} \right) \left(
                 e_{t-k} - \frac{1}{2}e_{t-1-k} \right)] \\
                 &= 0 - \frac{1}{2} \cdot 0 - \frac{1}{2} \cdot  0 +
                 \frac{1}{4} \cdot 0 \\
                 &= 0\\
    \end{aligned}
\end{equation*}

\subsection{More examples, introducing the autoregressive process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=929c1592-39cf-4d0d-9678-cdb3bf6f278b}{Time Series Video 11}}

We'll introduce the autoregressive (AR) process as having parameter $|\phi| <
1$ and being a time series like:
\begin{equation*}
    y_t = \phi y_{t-1} + e_t 
\end{equation*}
Which implies that the expectation can be calculated as:
\begin{equation*}
    \mathbb{E}[y_t] = \dots = \phi \mathbb{E}[y_{t-1}] 
\end{equation*}
So now we've got a self-referencial statement where the expectation at time
$t$ depends on the expectation at time $t-1$. This can collapse to become:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \lim_{h\to\infty} \phi^h \mathbb{E}[y_{t-h}]  \\
            &= 0 \cdot \mathbb{E}[y_{t-h}]  \\
            &= 0 \\
    \end{aligned}
\end{equation*}
And the variance will similarily be self-referencial, and collapse to give us:
\begin{equation*}
    \mathbb{V}[y_t] = \dots = \phi^{2h} \mathbb{V}[y_{t-h}] + \sigma^2
    \sum_{i=0}^{h-1} \phi^{2i}
\end{equation*}
And so taking the limit as $h\to\infty$:
 \begin{equation*}
     \begin{aligned}
         \mathbb{V}[y_t] &= \sigma^2 \sum_{i=0}^{h-1} \phi^{2i}\\
                         &= \frac{\sigma^2}{1 - \phi^2} \\
     \end{aligned}
\end{equation*}

\section{Autocorrelation}
Given a certain time series like $X_t = (x_1, \dots, x_n)$ we can
calculate the autocorrelation for offset $m$ like so:
\begin{equation*}
    \hat{\rho}_m = \mathbb{C}orr[X_t, X_{t-m}] = \frac{
        \sum_{t=m+1}^{n} \left( X_t - \bar{X} \right) \left( X_{t-m} - \bar{X} \right)
        }{
        \sum_{t=1}^{n} \left( X_t - \bar{X}\right)^2
        }
\end{equation*}
A \textbf{Correlogram} is a plot with $m$ on the x axis and the $m$-th lagged
autocorrelations on the y axis.

\section{Significance testing of Autocorrelation}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=7322200d-179b-4704-970c-e17c8cc40f66}{Time Series Video 12}}
Note that when you calculate autocorrelation with an offset of $m$, you'll lose
exactly $m$ data points. For example, in calculating the autocorrelation with
$m=1$, there is no $Z_{t_0}$ data point to match up with the  $Z_{t_1}$ data
point.

Today we look at the statistical significance of each of the $m$
autocorrelations. If the underlying process is a moving average process, then
the number of statistically significant autocorrelations is equal to the order
of that moving average process.

Recall that the $m$-th autocorrelation is defined as:
\begin{equation*}
    \hat{\rho}_m = \frac{
        \sum_{t=m+1}^{n} \left( X_t - \bar{X} \right) \left( X_{t-m} - \bar{X} \right)
        }{
        \sum_{t=1}^{n} \left( X_t - \bar{X}\right)^2
        }
\end{equation*}
Now we can form a standard normal random variable by subtracting the mean and
dividing by the sample variation. We have to find the distribution of
$\hat{\rho}_m$ numerically, by simulating the histogram many many times (No
easy analytical solution exists). After doing this, we find:
\begin{equation*}
    \hat{\rho}_m \sim N(0, \frac{1}{n})
\end{equation*}
And so we can construct a standard normal like so:
\begin{equation*}
    Z = \frac{\hat{\rho}_m - 0}{\frac{1}{\sqrt{n}}}
\end{equation*}

\subsection{Hypothesis tests with Autocorrelation}
We know that $\hat{\rho}_m \sim N(0, \frac{1}{n})$ (as $n\to\infty$).  Thus if
$|\hat{\rho}_m| > \frac{t_{\alpha/2}}{\sqrt{n}}$ we would reject $H_0: \rho_m =
0$ at the  $\alpha$ significance level (where $t_{\alpha/2}$ is the usual value
from student's t-distribution).

The \textbf{Bartlett's Test} is the same as above, except we reject $H_0$ if
$|\hat{\rho}_m| > \frac{2}{\sqrt{n}}$ because fuck accuracy.

The \textbf{Portmanteau test} allows us to also test more complicated
hypotheses such as:
\begin{equation*}
    \begin{aligned}
        H_0: \hat{\rho}_i &= 0 \quad \forall i \in \{1, 2, \dots, k\} \\
        H_1: \hat{\rho}_i &\ne 0 \quad \text{For at least one} i \in \{1, 2, \dots, k\} \\
    \end{aligned}
\end{equation*}
Recalling that the sum of squared independent standard normals is
chi-square distributed, we define the Portmanteau test for offset $m$ as:
\begin{equation*}
    \explain{Q(m) = n \sum_{i=1}^{m} \hat{\rho}_i^2}{$\sim \chi^2_m$}
\end{equation*}
So now we've got a chi-squared distribution, we can look up the value of $Q(m)$
in the tables.

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=3aadc357-4876-4b61-bb69-4d6ac7347aeb}{Time Series Video 13}}
\section{Linear Models for Time Series}
\subsection{White Noise Process}
This process is defined as only having an error term $e_t$ like:
 \begin{equation*}
     y_t = e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
So we can calculate the moments as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= 0 \\
        \mathbb{V}[y_t] &= \sigma^2 \\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = 0 \\
        \gamma_k &= \dots = 0 \\
    \end{aligned}
\end{equation*}

\subsection{The Random Walk Process}
This is defined as being dependant on the previous time step:
\begin{equation*}
    \begin{aligned}
        y_t &= y_{t-1} + e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$} \\
            &\text{And we (usually) assume $y_0$ as:} \\
        y_0 &= 0 \\
    \end{aligned}
\end{equation*}
Note that the errors are cumulative:
\begin{equation*}
    \begin{aligned}
        y_t &= y_0 + \sum_{i=1}^{t} e_i \\
            &= \sum_{i=1}^{t} e_i \\
    \end{aligned}
\end{equation*}
Recall that if $X_1 \indep X_2$ then:
\begin{equation*}
        \mathbb{E}[X_1 \cdot X_2] = \mathbb{E}[X_1] \cdot \mathbb{E}[X_2]
\end{equation*}

So the moments are given as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \dots = y_0 = 0  \\
        \mathbb{V}[y_t] &= \dots = t\sigma^2\\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = (t-1) \sigma^2 \\
                 &= \text{Note in general, if we have $s < t$:} \\
        \mathbb{C}ov[y_t, y_s] &= s \sigma^2 \\
        \gamma_k &= \dots = 0 \\
    \end{aligned}
\end{equation*}


\subsection{The Random Walk Process with Drift}
This is defined as being dependant on the previous time step, in addition to
some accumulative term:
\begin{equation*}
    \begin{aligned}
        y_t &= y_{t-1} + \mu + e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$} \\
            &\text{And we (usually) assume $y_0$ as:} \\
        y_0 &= 0 \\
    \end{aligned}
\end{equation*}

So the moments are given as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \dots = y_0 + t\mu  \\
        \mathbb{V}[y_t] &= \dots = t\sigma^2\\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = (t-1) \sigma^2 \\
                 &= \text{Note in general, if we have $s < t$:} \\
        \mathbb{C}ov[y_t, y_s] &= s \sigma^2 \\
    \end{aligned}
\end{equation*}
Define the \textbf{difference operator} as having symbol $\nabla$ and being
like:
\begin{equation*}
    \nabla y_t := y_t - y_{t-1} = \explain{\mu + e_t}{For a random walk with
    drift}
\end{equation*}
Often the difference operator can help create a stationary process from a non
stationary one.

\section{First Order Moving Average process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=757a8720-2693-47b1-9068-e6352a7edd93}{Time Series Video 14}}
We'll start talking about first order, then second order, and then we'll
abstract to talking about the $q$-th order moving average process. For now we
talk about the first order moving average process and define it as:
\begin{equation*}
    Z_t = e_t - \theta e_{t-1} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
Where $\theta$ is some weight that we can choose. Note that the sign between
the two terms is irrelevant, as we can choose $\theta$ to counteract whatever
sign we choose.

We can then find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \gamma_0 &= \mathbb{V}[Z_t] = \sigma^2(1 + \theta^2) \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= 
           \begin{cases}
               \sigma^2(1 + \theta^2) & \text{For $k=0$} \\
                   -\theta\sigma^2 & \text{For $k=1$} \\
                   0  & \text{For $k>1$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
                1 & \text{$k = 0$}\\
                -\frac{\theta}{1 + \theta^2} & \text{$k = 1$}\\
                0  & \text{For $k>1$} \\
            \end{cases}
    \end{aligned}
\end{equation*}

\section{Second Order Moving Average Process}
\begin{equation*}
    Z_t = e_t - \theta_1 e_{t-1} - \theta_2 e_{t-2} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
We can find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= \begin{cases}
               \theta^2 (1 + \theta_1^2 + \theta_2^2) & \text{For $k=0$} \\
                   \sigma^2\theta_1(\theta_2 - 1) & \text{For $k=1$} \\
                   -\theta_2\sigma^2 & \text{For $k=2$} \\
                   0  & \text{For $k>2$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
            1  \quad &k= 0 \\
            \frac{\theta_1(\theta_2 - 1)}{1 + \theta_1^2 + \theta_2^2} & k=1 \\
            \frac{-\theta_2}{1 + \theta_1^2 + \theta^2_2} & k=2 \\
            0 & k>2 \\
            \end{cases}
    \end{aligned}
\end{equation*}

\section{Q-th Order Moving Average Process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=372248c4-2710-4c42-a7df-ff73b1eabcf1}{Time Series Video 15}}
\begin{equation*}
    Z_t = e_t - \sum_{i=1}^{q} \theta_i e_{t-i} \qquad \text{Where $e_t \iid
    N(0, \sigma^2)$}
\end{equation*}

We can find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \mathbb{V}[Z_t] & = \sigma^2 + \sigma^2 \sum_{i=1}^{q} \theta^2_i \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= \begin{cases}
               -\sigma^2\theta_k + \sigma^2 \sum_{i=1}^{q}
               \theta_{k+i}\theta_i &  \text{For $1 \le k \le q$} \\
               0 & \text{For $k > q$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
            \frac{
                -\theta_k + \sum_{i=1}^{q} \theta_{k+i}\theta_i
            }{
                1 + \sum_{i=1}^{q} \theta_i^2
            } &  \text{For $1 \le k \le q$} \\
               0 & \text{For $k > q$} \\
            \end{cases}
    \end{aligned}
\end{equation*}

\section{Topics to be covered in the future}
\subsection{Autoregressive Process}
\subsection{Autoregressive Moving Average Process}
\subsection{ARIMA Process}
