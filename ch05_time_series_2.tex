\section{3041F: Time Series II Intro}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=41fa68b6-33c6-479e-b4a6-9909d6ec7edb}{Time Series Video 8}}
We'll be covering:
\begin{itemize}
    \item Stationarity, strict and weak variants.
    \item Autocorrelation: Measure the same variable but at different time points, and calculate the correlation between those timepoints
    \item Autocovariance
    \item Linear Time series models (moving average, autoregressive process, autoregressive moving average process)
\end{itemize}  

\subsection{Backshift Operator}
We'll (eventually) need the \textbf{Backshift Operator}, so define it as:
\begin{enumerate}
    \item \begin{equation*}
            B^iy_t = y_{t-i}
        \end{equation*}
    \item \begin{equation*}
            Bc = c
        \end{equation*}
    \item \begin{equation*}
            (B^i + B^j)y_t = B^iy_t + B^jy_t = y_{t-i} + y_{t-j}
        \end{equation*}
    \item \begin{equation*}
            B^iB^jy_t = B^jB^iy_t = B^iy_{t-j}= y_{t-j-i}   
        \end{equation*}
    \item Also, specially define the following:\begin{equation*}
            \theta(B) = 1 - \theta_1B - \theta_2B^2 - \dots - \theta_qB^q
    \end{equation*}
    \item For $|a| < 1$: \begin{equation*}
            (1 + aB + a^2B^2 + \dots )y_t = \frac{y_t}{1 - aB}
        \end{equation*}
    \item For $|a| > 1$: \begin{equation*}
            (1 + \frac{1}{aB} + \frac{1}{a^2B^2} + \dots )y_t = \frac{-aBy_t}{1
            - aB}
        \end{equation*}
    
\end{enumerate}
\section{Stationarity}
\subsection{Strict Stationarity}
Strict stationarity is a property of a time series.
\begin{itemize}
    \item A time series is strictly stationary iff for every offset parameter
        $m > 0$, all time points $t_1, \dots, t_n$, the joint distribution of
        $Z_{t_1}, \dots, Z_{t_n}$ is equal to that of $Z_{t_1 - m},
        \dots, Z_{t_n-m}$.
    \item We also require that if there are two points, $Z_t$ and  $Z_{t+m}$
        then  the distribution of the random vector  $\left( Z_t,
        Z_{t+m}\right)$ must be a function of the \textbf{lag} $|m|$.
    \item Given a timeline of a time series and an offset parameter $m$ 
    \item Consider Random Variables $Z_{t_1}, \dots, Z_{t_n}$, find the joint
        distribution of them all, then apply a timeshift by the offset
        parameter $m$: $Z_{t_{1-m}}, \dots, Z_{t_{n-m}}$.
    \item then if the joint distribution of the first set of Random variables
        is equal to that the joint distribution of the second set of random
        variables, then the time series is said to be \textbf{Strictly
        Stationary}.
\end{itemize}

\subsection{Autocovariance}
Define the \textbf{Autocovariance} as the covariance of the same random
variable but at different time points:
\begin{equation*}
    \begin{aligned}
        \mathbb{C}ov[Z_t, Z_{t+m}] &:= \gamma_{|m|} \\ 
                &= \mathbb{E}[Z_t \cdot Z_{t+m}] - \mathbb{E}[Z_t] \cdot
                \mathbb{E}[Z_{t+m}] \\
        &\text{Also note that:} \\
        \gamma_m = \gamma_{-m}
        \mathbb{V}[Z_t] &= \gamma_0 \\
    \end{aligned}
\end{equation*}
We can use $|m|$ because Covariance doesn't depend on the order of
the random variables given to it. 

Define the \textbf{Autocorrelation} as the autocovariance divided by the
variance of a time series. The autocorrelation (given the offset parameter
$m$) $\rho_m$ is defined by:
\begin{equation*}
    \begin{aligned}
        \rho_m &= \frac{\gamma_m}{\gamma_0} \\
        \rho_0 &= \frac{\gamma_0}{\gamma_0} = 1 \\
    \end{aligned}
\end{equation*}


\subsection{Weak Stationarity}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=c9dbfa7f-7ba8-4145-8037-26bdd235fd17}{Time Series Video 9}}

Weak stationarity is defined by orders (order 1, order 2, etc). So to have weak
stationarity of order $n$, the time series must have equal $n$-th moments.
For example, weak stationarity of order 1 requires a constant expectation.

Weak Stationarity of order 2 requires 
\begin{itemize}
    \item A constant expectation.
    \begin{equation*}
        \mathbb{E}[Z_t] = \mathbb{E}[Z_{t-m}]  \forall m
    \end{equation*}
        
    \item the autocovariance to only depend on the offset parameter $m$. For
        example, where $t$ and $s$ are valid indices to the time series $Z$:
    \begin{equation*}
        \mathbb{C}ov[Z_t, Z_{t-m}] = \mathbb{C}ov[Z_s, Z_{s-m}] 
    \end{equation*}
\end{itemize}
Therefore Weak Stationary of order 2 implies Weak Stationarity of order 1.

\subsection{Some examples of Weak stationarity}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=364aeb35-5750-4eb1-b844-93a239f8f81a}{Time Series Video 10}}
If we define a time series as $y_t = e_t - \frac{1}{2}e_{t-1}$ with all error
terms independant and $e_t \sim N(0, \sigma^2)$ then:

\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \mathbb{E}[e_t - \frac{1}{2}e_{t-1}] = 0 + 0 = 0 \\
        \gamma_0 &= \mathbb{E}[\left( e_t - \frac{1}{2}e_{t-1}\right)] - \mathbb{E}[y_t] \mathbb{E}[y_t]  \\
                 &= \dots \\
                 &= \sigma^2(1 + 0.25) + 0 \cdot 0 \\
                 &= 1.25 \sigma^2 \\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] \\
                 &= \mathbb{E}[\left( y_t \right) \left( y_{t-1}\right)] -
                 0 \cdot 0\\
                 &= \dots \\
                 &= -0.5\sigma^2 \\
        \gamma_k &= \mathbb{E}[y_t \cdot y_{t-k}]  - 0 \cdot 0\\
                 &= \mathbb{E}[\left( e_t - \frac{1}{2}e_{t-1} \right) \left(
                 e_{t-k} - \frac{1}{2}e_{t-1-k} \right)] \\
                 &= 0 - \frac{1}{2} \cdot 0 - \frac{1}{2} \cdot  0 +
                 \frac{1}{4} \cdot 0 \\
                 &= 0\\
    \end{aligned}
\end{equation*}

\subsection{More examples, introducing the autoregressive process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=929c1592-39cf-4d0d-9678-cdb3bf6f278b}{Time Series Video 11}}

We'll introduce the autoregressive (AR) process as having parameter $|\phi| <
1$ and being a time series like:
\begin{equation*}
    y_t = \phi y_{t-1} + e_t 
\end{equation*}
Which implies that the expectation can be calculated as:
\begin{equation*}
    \mathbb{E}[y_t] = \dots = \phi \mathbb{E}[y_{t-1}] 
\end{equation*}
So now we've got a self-referencial statement where the expectation at time
$t$ depends on the expectation at time $t-1$. This can collapse to become:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \lim_{h\to\infty} \phi^h \mathbb{E}[y_{t-h}]  \\
            &= 0 \cdot \mathbb{E}[y_{t-h}]  \\
            &= 0 \\
    \end{aligned}
\end{equation*}
And the variance will similarily be self-referencial, and collapse to give us:
\begin{equation*}
    \mathbb{V}[y_t] = \dots = \phi^{2h} \mathbb{V}[y_{t-h}] + \sigma^2
    \sum_{i=0}^{h-1} \phi^{2i}
\end{equation*}
And so taking the limit as $h\to\infty$:
 \begin{equation*}
     \begin{aligned}
         \mathbb{V}[y_t] &= \sigma^2 \sum_{i=0}^{h-1} \phi^{2i}\\
                         &= \frac{\sigma^2}{1 - \phi^2} \\
     \end{aligned}
\end{equation*}

\section{Autocorrelation}
Given a certain time series like $X_t = (x_1, \dots, x_n)$ we can
calculate the autocorrelation for offset $m$ like so:
\begin{equation*}
    \hat{\rho}_m = \mathbb{C}orr[X_t, X_{t-m}] = \frac{
        \sum_{t=m+1}^{n} \left( X_t - \bar{X} \right) \left( X_{t-m} - \bar{X} \right)
        }{
        \sum_{t=1}^{n} \left( X_t - \bar{X}\right)^2
        }
\end{equation*}
A \textbf{Correlogram} is a plot with $m$ on the x axis and the $m$-th lagged
autocorrelations on the y axis.

\section{Significance testing of Autocorrelation}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=7322200d-179b-4704-970c-e17c8cc40f66}{Time Series Video 12}}
Note that when you calculate autocorrelation with an offset of $m$, you'll lose
exactly $m$ data points. For example, in calculating the autocorrelation with
$m=1$, there is no $Z_{t_0}$ data point to match up with the  $Z_{t_1}$ data
point.

Today we look at the statistical significance of each of the $m$
autocorrelations. If the underlying process is a moving average process, then
the number of statistically significant autocorrelations is equal to the order
of that moving average process.

Recall that the $m$-th autocorrelation is defined as:
\begin{equation*}
    \hat{\rho}_m = \frac{
        \sum_{t=m+1}^{n} \left( X_t - \bar{X} \right) \left( X_{t-m} - \bar{X} \right)
        }{
        \sum_{t=1}^{n} \left( X_t - \bar{X}\right)^2
        }
\end{equation*}
Now we can form a standard normal random variable by subtracting the mean and
dividing by the sample variation. We have to find the distribution of
$\hat{\rho}_m$ numerically, by simulating the histogram many many times (No
easy analytical solution exists). After doing this, we find:
\begin{equation*}
    \hat{\rho}_m \sim N(0, \frac{1}{n})
\end{equation*}
And so we can construct a standard normal like so:
\begin{equation*}
    Z = \frac{\hat{\rho}_m - 0}{\frac{1}{\sqrt{n}}}
\end{equation*}

\subsection{Hypothesis tests with Autocorrelation}
We know that $\hat{\rho}_m \sim N(0, \frac{1}{n})$ (as $n\to\infty$).  Thus if
$|\hat{\rho}_m| > \frac{t_{\alpha/2}}{\sqrt{n}}$ we would reject $H_0: \rho_m =
0$ at the  $\alpha$ significance level (where $t_{\alpha/2}$ is the usual value
from student's t-distribution).

The \textbf{Bartlett's Test} is the same as above, except we reject $H_0$ if
$|\hat{\rho}_m| > \frac{2}{\sqrt{n}}$ because fuck accuracy.

The \textbf{Portmanteau test} allows us to also test more complicated
hypotheses such as:
\begin{equation*}
    \begin{aligned}
        H_0: \hat{\rho}_i &= 0 \quad \forall i \in \{1, 2, \dots, k\} \\
        H_1: \hat{\rho}_i &\ne 0 \quad \text{For at least one} i \in \{1, 2, \dots, k\} \\
    \end{aligned}
\end{equation*}
Recalling that the sum of squared independent standard normals is
chi-square distributed, we define the Portmanteau test for offset $m$ as:
\begin{equation*}
    \explain{Q(m) = n \sum_{i=1}^{m} \hat{\rho}_i^2}{$\sim \chi^2_m$}
\end{equation*}
So now we've got a chi-squared distribution, we can look up the value of $Q(m)$
in the tables.

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=3aadc357-4876-4b61-bb69-4d6ac7347aeb}{Time Series Video 13}}
\section{Linear Models for Time Series}
\subsection{White Noise Process}
This process is defined as only having an error term $e_t$ like:
 \begin{equation*}
     y_t = e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
So we can calculate the moments as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= 0 \\
        \mathbb{V}[y_t] &= \sigma^2 \\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = 0 \\
        \gamma_k &= \dots = 0 \\
    \end{aligned}
\end{equation*}

\subsection{The Random Walk Process}
This is defined as being dependant on the previous time step:
\begin{equation*}
    \begin{aligned}
        y_t &= y_{t-1} + e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$} \\
            &\text{And we (usually) assume $y_0$ as:} \\
        y_0 &= 0 \\
    \end{aligned}
\end{equation*}
Note that the errors are cumulative:
\begin{equation*}
    \begin{aligned}
        y_t &= y_0 + \sum_{i=1}^{t} e_i \\
            &= \sum_{i=1}^{t} e_i \\
    \end{aligned}
\end{equation*}
Recall that if $X_1 \indep X_2$ then:
\begin{equation*}
        \mathbb{E}[X_1 \cdot X_2] = \mathbb{E}[X_1] \cdot \mathbb{E}[X_2]
\end{equation*}

So the moments are given as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \dots = y_0 = 0  \\
        \mathbb{V}[y_t] &= \dots = t\sigma^2\\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = (t-1) \sigma^2 \\
                 &= \text{Note in general, if we have $s < t$:} \\
        \mathbb{C}ov[y_t, y_s] &= s \sigma^2 \\
        \gamma_k &= \dots = 0 \\
    \end{aligned}
\end{equation*}


\subsection{The Random Walk Process with Drift}
This is defined as being dependant on the previous time step, in addition to
some accumulative term:
\begin{equation*}
    \begin{aligned}
        y_t &= y_{t-1} + \mu + e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$} \\
            &\text{And we (usually) assume $y_0$ as:} \\
        y_0 &= 0 \\
    \end{aligned}
\end{equation*}

So the moments are given as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \dots = y_0 + t\mu  \\
        \mathbb{V}[y_t] &= \dots = t\sigma^2\\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = (t-1) \sigma^2 \\
                 &= \text{Note in general, if we have $s < t$:} \\
        \mathbb{C}ov[y_t, y_s] &= s \sigma^2 \\
    \end{aligned}
\end{equation*}
Define the \textbf{difference operator} as having symbol $\nabla$ and being
like:
\begin{equation*}
    \nabla y_t := y_t - y_{t-1} = \explain{\mu + e_t}{For a random walk with
    drift}
\end{equation*}
Often the difference operator can help create a stationary process from a non
stationary one.

\section{First Order Moving Average process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=757a8720-2693-47b1-9068-e6352a7edd93}{Time Series Video 14}}
We'll start talking about first order, then second order, and then we'll
abstract to talking about the $q$-th order moving average process. For now we
talk about the first order moving average process and define it as:
\begin{equation*}
    Z_t = e_t - \theta e_{t-1} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
Where $\theta$ is some weight that we can choose. Note that the sign between
the two terms is irrelevant, as we can choose $\theta$ to counteract whatever
sign we choose.

We can then find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \gamma_0 &= \mathbb{V}[Z_t] = \sigma^2(1 + \theta^2) \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= 
           \begin{cases}
               \sigma^2(1 + \theta^2) & \text{For $k=0$} \\
                   -\theta\sigma^2 & \text{For $k=1$} \\
                   0  & \text{For $k>1$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
                1 & \text{$k = 0$}\\
                -\frac{\theta}{1 + \theta^2} & \text{$k = 1$}\\
                0  & \text{For $k>1$} \\
            \end{cases}
    \end{aligned}
\end{equation*}

\section{Second Order Moving Average Process}
\begin{equation*}
    Z_t = e_t - \theta_1 e_{t-1} - \theta_2 e_{t-2} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
We can find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= \begin{cases}
               \theta^2 (1 + \theta_1^2 + \theta_2^2) & \text{For $k=0$} \\
                   \sigma^2\theta_1(\theta_2 - 1) & \text{For $k=1$} \\
                   -\theta_2\sigma^2 & \text{For $k=2$} \\
                   0  & \text{For $k>2$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
            1  \quad &k= 0 \\
            \frac{\theta_1(\theta_2 - 1)}{1 + \theta_1^2 + \theta_2^2} & k=1 \\
            \frac{-\theta_2}{1 + \theta_1^2 + \theta^2_2} & k=2 \\
            0 & k>2 \\
            \end{cases}
    \end{aligned}
\end{equation*}

\section{Q-th Order Moving Average Process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=372248c4-2710-4c42-a7df-ff73b1eabcf1}{Time Series Video 15}}
\begin{equation*}
    Z_t = e_t - \sum_{i=1}^{q} \theta_i e_{t-i} \qquad \text{Where $e_t \iid
    N(0, \sigma^2)$}
\end{equation*}

We can find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \mathbb{V}[Z_t] & = \sigma^2 + \sigma^2 \sum_{i=1}^{q} \theta^2_i \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= \begin{cases}
               -\sigma^2\theta_k + \sigma^2 \sum_{i=1}^{q}
               \theta_{k+i}\theta_i &  \text{For $1 \le k \le q$} \\
               0 & \text{For $k > q$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
            \frac{
                -\theta_k + \sum_{i=1}^{q} \theta_{k+i}\theta_i
            }{
                1 + \sum_{i=1}^{q} \theta_i^2
            } &  \text{For $1 \le k \le q$} \\
               0 & \text{For $k > q$} \\
            \end{cases}
    \end{aligned}
\end{equation*}

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=f3d183b3-57bf-45c4-8899-d32d7b58616f}{Time Series Video 16}}
\section{General Linear Process}
\textbf{General Linear Process}: A weighted infinite sum of error terms, where
we call the ith weight $\psi_i$.
\begin{equation*}
    \begin{aligned}
        \sum_{i=1}^{\infty} \psi_i^2 &< \infty \\
        |\psi_i| &< 1 \\
        \psi_0 &= 1 \\
        Z_t &= \sum_{i=0}^{\infty} \psi_i \cdot e_{t-i} \\
    \end{aligned}
\end{equation*}
For example, we can set $\psi_i = (\frac{1}{2})^2$. This obeys the
requirements.

With a question like \textbf{Identify this process}, you just need to say if
it's a General Linear Process, a moving average process, etc.

With the example linear process like: $\psi_i = \phi^i$ and with $\phi_i \in
(0, 1)$ we've got a fully define process given by:

\begin{equation*}
    \begin{aligned}
        Z_t &= \sum_{i=0}^{\infty} \phi^i e_{t-i} \\
        \gamma_0 &= \mathbb{E}[Z_t^2]\\
                 &= \sigma^2 \sum_{i=0}^{\infty} \phi^{2i}\\
                 &= \frac{\sigma^2}{1 - \phi^2}
        \gamma_k &= \frac{\phi^k \sigma^2}{1 - \phi^2} \qquad \forall k=0,1,.. \\
        \rho_k &= \phi^k \qquad \forall k=0,1,.. \\
    \end{aligned}
\end{equation*}

An example of calculating the autocovariance $\gamma_k$ :
\begin{equation*}
    \begin{aligned}
        \gamma_k &= \mathbb{C}ov[Z_t, Z_{t+k}] = \mathbb{E}[Z_t \cdot Z_{t_k}] \\
                 &= \mathbb{E}[\left( \sum_{i=0}^{\infty} \phi^i e_{t-i}\right)
                 \left(\sum_{j=0}^{\infty} \phi^j e_{t+k-j} \right)] \\
                 &\text{let } t - i = t + k - j \iff j = i+k\\
        \gamma_k = \mathbb{E}[\left( \sum_{i=0}^{\infty} \phi^i e_{t-i}\right)
                \left(\sum_{i=0}^{\infty} \phi^{i+k} e_{t-i} \right)] \\
            &= \phi^k \mathbb{E}[\left( \sum_{i=0}^{\infty} \phi^i
                 e_{t-i}\right)^2] \\
            &= \frac{\phi^k \sigma^2}{1 - \phi^2} \\
    \end{aligned}
\end{equation*}
Note that general linear processes are weakly stationary of order 2 since
neither the mean nor the covariance depends on time $t$.


\section{Infinite Moving Average Process}
Very similar to the above General Linear Process:
\begin{equation*}
    Z_t = \sum_{i=0}^{\infty} \omega_i e_{t-i} \qquad \omega_0 = 1
\end{equation*}

\begin{equation*}
    \begin{aligned}
        \mathbb{E}[Z_t] &= 0 \\
        \gamma_0 &= \lim_{T\to\infty} \sigma^2 \sum_{i=0}^{T} \omega_i^2 \\
        \gamma_k &= \lim_{T\to\infty} \sigma^2 \sum_{i=0}^{T} \omega_i
        \omega_{k+i} \\
        \rho_k &= \lim_{T\to\infty} \frac{\sum_{i=0}^{T} \omega_i
        \omega_{k+i}}{sum_{i=0}^{T} \omega_i^2} \\
    \end{aligned}
\end{equation*}


\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=ea9eba37-1039-4ca0-a4ab-7178816a8345}{Time Series Video 17}}
Have previously looked at the moving average process.
Will now be looking at forecasts for moving average processes
\subsection{Forecasts for Moving Average Processes}
With the moving average process defined by:
\begin{equation*}
    Z_t = e_t - \phi e_{t-1} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
And now lets define the forecast value as $\hat{Z}_{t+1}$ and we set it equal
to be:
 \begin{equation*}
     \begin{aligned}
         \hat{Z}_{t+1} &= \mathbb{E}[Z_{t+1} | \explain{Z_t, Z_{t-1} \dots
         Z_1}{Often called just $I_t$, the information at  $t$}]  \\
                       &= \mathbb{E}[e_{t+1} - \phi e_t | I_t]  \\
                       &= \explain{\mathbb{E}[e_{t+1} | I_t]}{$e_{t-1}$ is
                       still a random variable} - \phi \explain{\mathbb{E}[e_t
                   | I_t]}{$e_t$ has a measured value}  \\
                       &= 0 - \phi e_t
                       &= -\phi e_t
     \end{aligned}
\end{equation*}
So the forecast for a moving average process is $ \hat{Z}_{t+1} = -\phi e_t$.

We often also want to know \textbf{the margin of error}, so we define the one
step ahead forecast error $ \hat{e}(1)$ as:
\begin{equation*}
    \begin{aligned}
        \hat{e}(1) &= Z_{t+1} - \hat{Z}_{t+1} \\
                   &= (e_{t+1} - \phi e_t) - (- \phi e_t) \\
                   &= e+{t+1}
                   &\text{Note that we can also calculate:}\\
        \mathbb{V}[ \hat{e}(1)] &= \sigma^2 \\
    \end{aligned}
\end{equation*}

Now, for a \textbf{MA(q) model} , we have:
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+l} &= - \sum_{i=l}^{q} \phi_i e_{t-i+l} \qquad \forall 1 
        \le l \le q \\
        \hat{e}(l) &= Z_{t+l} - \hat{Z}_{t+l} \\
            \mathbb{V}[ \hat{e}(l)] &= \begin{cases}
            \sigma^2 & \text{for $l=1$}\\
            \sigma^2 \left( 1 + \sum_{i=1}^{l-1} \phi_i^2 \right)& \text{for $
            2 \le l \le q$}\\
            \sigma^2 \left( 1 + \sum_{i=1}^{q} \phi_i^2 \right)& \text{for $l >
            q$}\\
        \end{cases}
    \end{aligned}
\end{equation*}


\subsection{Autoregressive Process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?id=35d8e9d3-94b8-4149-90d1-e85a0c8c11da&ltimode=true}{Time Series Video 18}}
A first order autoregressive process (AR(1))can be defined as a weighted linear
sum of the previous value in the time series $Z_{t-1}$ and an error term
$e_{t}$:
\begin{equation*}
    Z_t = \phi Z_{t - 1} + e_{t} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
Since $Z_{t}$ depends on $Z_{t-1}$, this can be expanded into an infinite
moving average process (assuming $|\phi| < 1$:
\begin{equation*}
    Z_{t} = \sum_{i=0}^{\infty} \phi^{i} e_{t-1}
\end{equation*}
So some properties are given by:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[Z_{t}] &= 0 \\
        \mathbb{V}[Z_{t}] &= \frac{\sigma^{2}}{1 - \phi^{2}}\\
        \gamma_{k} &= \frac{\phi^{k}\sigma^{2}}{1-\phi^{2}} = \phi^{k}
        \gamma_0  \\
        \rho_{k} &= \phi^{k} \\
    \end{aligned}
\end{equation*}

We can also write the AR(1) process as:
\begin{equation*}
    \begin{aligned}
        e_{t} &= Z_{t} - \phi Z_{t-1} = (1 - \phi B) Z_{t}\\ 
        &= \phi(B)Z_{t} \\ 
    \end{aligned}
\end{equation*}


\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?id=3a8d235f-b875-4370-92aa-25d86f869e8d&ltimode=true}{Time Series Video 19}}
Now we'll write the AR(2) process as:
\begin{equation*}
    \begin{aligned}
        Z_{t} &= \phi_{1}Z_{t-1} + \phi_{2}Z_{t-2} + e_t \qquad \text{Where $e_t
        \iid N(0, \sigma^2)$} \\
              &\text{Or alternatively:}\\
            (1 - \phi_{1}B - \phi_{2}B^{2})Z_{t} &= e_{t} \\
    \end{aligned}
\end{equation*}
And if we assume the time series is weakly stationary of order 2, and that
$\sum_{i=1}^{2} \phi_{i} \ne 1$: 
(\textbf{WS(2)}) then:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[Z_{t}] &= 0 \\
        \gamma_{0} &= \left( \frac{1 - \phi_{2}}{1 + \phi_{2}} \right) \frac{\sigma^{2}}{1 - \phi^{2}_{2} - \phi_{1}^{2}} \\
        \gamma_{k} &= \phi_{1}\gamma_{k-1} + \phi_{2}\gamma_{k-2} \\
        \rho_{k} &= \phi_{1}\rho_{k-1} + \phi_{2}\rho_{k-2} \\
                          &\text{Where $\rho_{-1} = \rho_{1}$ and $\rho_{0}=1$.}\\
   \end{aligned}
\end{equation*}
The above equations for $\gamma_{k}$ and $\rho_{k}$ are called the
\textbf{Yule-Walker} equations.

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?id=61de3aff-2f8f-4535-973e-f7539d887186&ltimode=true}{Time Series Video 20}}
\subsection{The Characteristic Equation}
Taking the formula for a AR(2) process, we can write it as:
\begin{equation*}
    \begin{aligned}
        Z_{t} &= \phi_{1}Z_{t-1} + \phi_{2}Z_{t-2} + e_t \\
        e_{t} &= Z_{t}\left( \explain{1 - \phi_{1}B - \phi_{2}B^{2}}{Equate
        this to zero} \right) \\
                1 - \phi_{1}B - \phi_{2}B^{2} &= 0 \\
                \text{Let $x = \frac{1}{B}$} &\\
                x^{2} - \phi_{1}x - \phi_{2} &= 0 \\
                x &= \frac{\phi_{1} \pm \sqrt{\phi^{2}_{1} + 4\phi_{2}}}{2}
    \end{aligned}
\end{equation*}

In general, an \textbf{AA(q) process is weakly stationary of order 2 iff} the
characteristic roots lie inside the unit circle in the complex plane, ie:
\begin{equation*}
    \begin{aligned}
        \phi_{1} + \phi_{2} &< 1 \\
        \phi_{2} - \phi_{1} &< 1 \\
        |\phi_{2}| &< 1 \\
    \end{aligned}
\end{equation*}

\subsection{P-th order Autoregressive Process}
The p-th order autoregressive process AR(p) can be written as a linear
combination of the previous p time points, plus some random error.
\begin{equation*}
    \begin{aligned}
        Z_{t} = e_{t} + \sum_{i=1}^{p} \phi_{i}Z_{t-i} \qquad \text{Where $e_t
        \iid N(0, \sigma^2)$}
    \end{aligned}
\end{equation*}

Then the lag-k autocovariance $\gamma_{k}$ can be found by:
\begin{equation*}
    \begin{aligned}
        \gamma_{k} &= \mathbb{E}[Z_{t} \cdot Z_{t-k}] \\
        Z_{t} \cdot Z_{t-k} &= Z_{t-k} \cdot e_{t} + Z_{t-k}\cdot\sum_{i=1}^{p}
        \phi_{i}Z_{t-i} \\
        \gamma_{k} &= \mathbb{E}[Z_{t-k} \cdot e_{t}] + Z_{t-k}\cdot\sum_{i=1}^{p}
        \phi_{i}Z_{t-i}] \\
        \gamma_{k} &= \mathbb{E}[Z_{t-k} \cdot e_{t}]
        +\mathbb{E}\left[Z_{t-k}\cdot\sum_{i=1}^{p} \phi_{i}Z_{t-i}\right] \\
    \end{aligned}
\end{equation*}
And now we can label the following equations as the Yule-Walker Equations:
\begin{equation*}
    \begin{aligned}
    \gamma_{k} &= 0 + \sum_{i=1}^{p} \phi_{i}\gamma_{k-i} \\
    \rho_{k} &= \sum_{i=1}^{p} \phi_{i}\phi_{k-i} \\
    \end{aligned}
\end{equation*}

\subsection{Finding the Order of an Autocorrelation Model}
The aim is to use data we've measured to find the best value for $p$, and to
evaluate how good this best value is.
We also define 
\begin{itemize}
    \item \textbf{ACF}: Autocorrelation Function and 
    \item \textbf{PACF}: Partial Autocorrelation Function
\end{itemize}

And then the lag-i partial autocorrelation is defined as $\phi_{ii}$ with the
double subscript to differentiate from the coefficients of the autoregressive
process, $\phi_{i}$.

The partial autocorrelation between $Z_{t}$ and $Z_{t-k}$ is the correlation
between $Z_{t}$ and $Z_{t-k}$ but with all the in between correlations from
$Z_{t-1}, \dots Z_{t-k-1}$ removed. This is done by subtracting a linear
combination of all the in between time series points, with the coefficients are
assigned as $\alpha_{1}, \dots , \alpha_{k-1}$. To make this explicit:
\begin{equation*}
    \phi_{ii} = \mathbb{C}orr\left[Z_{t} - \sum_{i=1}^{k-1} \alpha_{i}
    Z_{t-i},\quad Z_{t-k} -
\sum_{i=1}^{k-1} \alpha_{i} Z_{t-i} \right]
\end{equation*}

\subsubsection{Mean Square Error estimation of $\phi_{ii}$}
To solve for $\phi_{ii}$, note how we can use FOIL when calculating
covariance:
\begin{equation*}
    \mathbb{C}ov\left[A - B, C - D\right] = \mathbb{C}ov\left[A, C\right] -
    \mathbb{C}ov\left[A, D\right] - \mathbb{C}ov\left[B, C\right] +
    \mathbb{C}ov\left[B, D\right] 
\end{equation*}
But before we can use the above formula, we need estimates for $\alpha_{i}$.

We can define the MSE as the minimum value with respect to $ \alpha$ of:
\begin{equation*}
    \begin{aligned}
        \text{MSE} &= \mathbb{E}\left[(Z_{t} - \alpha Z_{t-1})^{2}\right] \\
        &= \mathbb{E}\left[Z_{t}^{2} - 2\alpha Z_{t}Z_{t-1} +
        \alpha^{2}Z_{t-1}^{2}\right]  \\
        &= \gamma_{0} - 2\alpha\gamma_{1} + \alpha^{2}\gamma_{0} \\
        &\text{Taking the derivative with respect to $\alpha$:}\\
        \frac{d\text{MSE}}{d\alpha} &= 0 \\
        -2\gamma_{1} + 2\alpha\gamma_{0} &= 0 \\
        \alpha &= \frac{2\gamma_{1}}{2\gamma_{0}} \\
               &= \rho_{1} \\
    \end{aligned}
\end{equation*}
So we now can calculate $\phi_{22}$ as:
\begin{equation*}
    \begin{aligned}
        \phi_{22} &= \mathbb{C}orr\left[Z_{t} - \alpha Z_{t-1},\quad Z_{t-2} -
        \alpha Z_{t-1} \right] \\
                  &= \frac{\gamma_{2} - \rho_{1} \gamma_{1} -
                  \rho_{1}\gamma_{1} + \rho_{2}^{2}\gamma_{0}}{
              \mathbb{V}\left[Z_{t} - \rho_{1} Z_{t-1}\right] } \\
                  &= \frac{\gamma_{2} - 2\rho_{1}\gamma_{1} +
                  \rho_{1}^{2}\gamma_{0}}{\gamma_{0} - 2\rho_{1}\gamma_{1} +
              \rho_{1}^{2}\gamma_{0}} \\
                  &\text{Note $\rho_{1} = \phi$, and $\rho_{2} = \phi \rho_{1}$}
                  &= \frac{\rho_{1} - 2 \rho_{1}^{2} + \rho_{1}^{2}}{1 - 2
                  \rho_{1}^{2} + \rho_{1}^{2}} \\
                  &= \frac{\phi^{2} - 2 \phi^{2} + \phi^{2}}{1 - 2\phi^{2} +
                  \phi^{2}} \\
                  &= 0 \\
    \end{aligned}
\end{equation*}
So the lag-2 partial autocorrelation is equal to zero. This is because the
autoregressive process is of order 1, \textbf{AR(1)} so any lag-n partial
autocorrelation with n greater than 1 will be zero.

\section{Topics to be covered in the future}

\subsection{Autoregressive Moving Average Process}
\subsection{ARIMA Process}
