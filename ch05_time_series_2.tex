\section{3041F: Time Series II Intro}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=41fa68b6-33c6-479e-b4a6-9909d6ec7edb}{Time Series Video 8}}
We'll be covering:
\begin{itemize}
    \item Stationarity, strict and weak variants.
    \item Autocorrelation: Measure the same variable but at different time points, and calculate the correlation between those timepoints
    \item Autocovariance
    \item Linear Time series models (moving average, autoregressive process, autoregressive moving average process)
\end{itemize}  

\section{Stationarity}
\subsection{Strict Stationarity}
Strict stationarity is a property of a time series.
\begin{itemize}
    \item A time series is strictly stationary iff for every offset parameter
        $m > 0$, all time points $t_1, \dots, t_n$, the joint distribution of
        $Z_{t_1}, \dots, Z_{t_n}$ is equal to that of $Z_{t_1 - m},
        \dots, Z_{t_n-m}$.
    \item We also require that if there are two points, $Z_t$ and  $Z_{t+m}$
        then  the distribution of the random vector  $\left( Z_t,
        Z_{t+m}\right)$ must be a function of the \textbf{lag} $|m|$.
    \item Given a timeline of a time series and an offset parameter $m$ 
    \item Consider Random Variables $Z_{t_1}, \dots, Z_{t_n}$, find the joint
        distribution of them all, then apply a timeshift by the offset
        parameter $m$: $Z_{t_{1-m}}, \dots, Z_{t_{n-m}}$.
    \item then if the joint distribution of the first set of Random variables
        is equal to that the joint distribution of the second set of random
        variables, then the time series is said to be \textbf{Strictly
        Stationary}.
\end{itemize}

\subsection{Autocovariance}
Define the \textbf{Autocovariance} as the covariance of the same random
variable but at different time points:
\begin{equation*}
    \begin{aligned}
        \mathbb{C}ov[Z_t, Z_{t+m}] &:= \gamma_{|m|} \\ 
                &= \mathbb{E}[Z_t \cdot Z_{t+m}] - \mathbb{E}[Z_t] \cdot
                \mathbb{E}[Z_{t+m}] \\
        &\text{Also note that:} \\
        \gamma_m = \gamma_{-m}
        \mathbb{V}[Z_t] &= \gamma_0 \\
    \end{aligned}
\end{equation*}
We can use $|m|$ because Covariance doesn't depend on the order of
the random variables given to it. 

Define the \textbf{Autocorrelation} as the autocovariance divided by the
variance of a time series. The autocorrelation (given the offset parameter
$m$) $\rho_m$ is defined by:
\begin{equation*}
    \begin{aligned}
        \rho_m &= \frac{\gamma_m}{\gamma_0} \\
        \rho_0 &= \frac{\gamma_0}{\gamma_0} = 1 \\
    \end{aligned}
\end{equation*}


\subsection{Weak Stationarity}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=c9dbfa7f-7ba8-4145-8037-26bdd235fd17}{Time Series Video 9}}

Weak stationarity is defined by orders (order 1, order 2, etc). So to have weak
stationarity of order $n$, the time series must have equal $n$-th moments.
For example, weak stationarity of order 1 requires a constant expectation.

Weak Stationarity of order 2 requires 
\begin{itemize}
    \item A constant expectation.
    \begin{equation*}
        \mathbb{E}[Z_t] = \mathbb{E}[Z_{t-m}]  \forall m
    \end{equation*}
        
    \item the autocovariance to only depend on the offset parameter $m$. For
        example, where $t$ and $s$ are valid indices to the time series $Z$:
    \begin{equation*}
        \mathbb{C}ov[Z_t, Z_{t-m}] = \mathbb{C}ov[Z_s, Z_{s-m}] 
    \end{equation*}
\end{itemize}
Therefore Weak Stationary of order 2 implies Weak Stationarity of order 1.

\subsection{Some examples of Weak stationarity}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=364aeb35-5750-4eb1-b844-93a239f8f81a}{Time Series Video 10}}
If we define a time series as $y_t = e_t - \frac{1}{2}e_{t-1}$ with all error
terms independant and $e_t \sim N(0, \sigma^2)$ then:

\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \mathbb{E}[e_t - \frac{1}{2}e_{t-1}] = 0 + 0 = 0 \\
        \gamma_0 &= \mathbb{E}[\left( e_t - \frac{1}{2}e_{t-1}\right)] - \mathbb{E}[y_t] \mathbb{E}[y_t]  \\
                 &= \dots \\
                 &= \sigma^2(1 + 0.25) + 0 \cdot 0 \\
                 &= 1.25 \sigma^2 \\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] \\
                 &= \mathbb{E}[\left( y_t \right) \left( y_{t-1}\right)] -
                 0 \cdot 0\\
                 &= \dots \\
                 &= -0.5\sigma^2 \\
        \gamma_k &= \mathbb{E}[y_t \cdot y_{t-k}]  - 0 \cdot 0\\
                 &= \mathbb{E}[\left( e_t - \frac{1}{2}e_{t-1} \right) \left(
                 e_{t-k} - \frac{1}{2}e_{t-1-k} \right)] \\
                 &= 0 - \frac{1}{2} \cdot 0 - \frac{1}{2} \cdot  0 +
                 \frac{1}{4} \cdot 0 \\
                 &= 0\\
    \end{aligned}
\end{equation*}

\subsection{More examples, introducing the autoregressive process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=929c1592-39cf-4d0d-9678-cdb3bf6f278b}{Time Series Video 11}}

We'll introduce the autoregressive (AR) process as having parameter $|\phi| <
1$ and being a time series like:
\begin{equation*}
    y_t = \phi y_{t-1} + e_t 
\end{equation*}
Which implies that the expectation can be calculated as:
\begin{equation*}
    \mathbb{E}[y_t] = \dots = \phi \mathbb{E}[y_{t-1}] 
\end{equation*}
So now we've got a self-referencial statement where the expectation at time
$t$ depends on the expectation at time $t-1$. This can collapse to become:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \lim_{h\to\infty} \phi^h \mathbb{E}[y_{t-h}]  \\
            &= 0 \cdot \mathbb{E}[y_{t-h}]  \\
            &= 0 \\
    \end{aligned}
\end{equation*}
And the variance will similarily be self-referencial, and collapse to give us:
\begin{equation*}
    \mathbb{V}[y_t] = \dots = \phi^{2h} \mathbb{V}[y_{t-h}] + \sigma^2
    \sum_{i=0}^{h-1} \phi^{2i}
\end{equation*}
And so taking the limit as $h\to\infty$:
 \begin{equation*}
     \begin{aligned}
         \mathbb{V}[y_t] &= \sigma^2 \sum_{i=0}^{h-1} \phi^{2i}\\
                         &= \frac{\sigma^2}{1 - \phi^2} \\
     \end{aligned}
\end{equation*}

\section{Autocorrelation}
Given a certain time series like $X_t = (x_1, \dots, x_n)$ we can
calculate the autocorrelation for offset $m$ like so:
\begin{equation*}
    \hat{\rho}_m = \mathbb{C}orr[X_t, X_{t-m}] = \frac{
        \sum_{t=m+1}^{n} \left( X_t - \bar{X} \right) \left( X_{t-m} - \bar{X} \right)
        }{
        \sum_{t=1}^{n} \left( X_t - \bar{X}\right)^2
        }
\end{equation*}
A \textbf{Correlogram} is a plot with $m$ on the x axis and the $m$-th lagged
autocorrelations on the y axis.

\section{Significance testing of Autocorrelation}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=7322200d-179b-4704-970c-e17c8cc40f66}{Time Series Video 12}}
Note that when you calculate autocorrelation with an offset of $m$, you'll lose
exactly $m$ data points. For example, in calculating the autocorrelation with
$m=1$, there is no $Z_{t_0}$ data point to match up with the  $Z_{t_1}$ data
point.

Today we look at the statistical significance of each of the $m$
autocorrelations. If the underlying process is a moving average process, then
the number of statistically significant autocorrelations is equal to the order
of that moving average process.

Recall that the $m$-th autocorrelation is defined as:
\begin{equation*}
    \hat{\rho}_m = \frac{
        \sum_{t=m+1}^{n} \left( X_t - \bar{X} \right) \left( X_{t-m} - \bar{X} \right)
        }{
        \sum_{t=1}^{n} \left( X_t - \bar{X}\right)^2
        }
\end{equation*}
Now we can form a standard normal random variable by subtracting the mean and
dividing by the sample variation. We have to find the distribution of
$\hat{\rho}_m$ numerically, by simulating the histogram many many times (No
easy analytical solution exists). After doing this, we find:
\begin{equation*}
    \hat{\rho}_m \sim N(0, \frac{1}{n})
\end{equation*}
And so we can construct a standard normal like so:
\begin{equation*}
    Z = \frac{\hat{\rho}_m - 0}{\frac{1}{\sqrt{n}}}
\end{equation*}

\subsection{Hypothesis tests with Autocorrelation}
We know that $\hat{\rho}_m \sim N(0, \frac{1}{n})$ (as $n\to\infty$).  Thus if
$|\hat{\rho}_m| > \frac{t_{\alpha/2}}{\sqrt{n}}$ we would reject $H_0: \rho_m =
0$ at the  $\alpha$ significance level (where $t_{\alpha/2}$ is the usual value
from student's t-distribution).

The \textbf{Bartlett's Test} is the same as above, except we reject $H_0$ if
$|\hat{\rho}_m| > \frac{2}{\sqrt{n}}$ because fuck accuracy.

The \textbf{Portmanteau test} allows us to also test more complicated
hypotheses such as:
\begin{equation*}
    \begin{aligned}
        H_0: \hat{\rho}_i &= 0 \quad \forall i \in \{1, 2, \dots, k\} \\
        H_1: \hat{\rho}_i &\ne 0 \quad \text{For at least one} i \in \{1, 2, \dots, k\} \\
    \end{aligned}
\end{equation*}
Recalling that the sum of squared independent standard normals is
chi-square distributed, we define the Portmanteau test for offset $m$ as:
\begin{equation*}
    \explain{Q(m) = n \sum_{i=1}^{m} \hat{\rho}_i^2}{$\sim \chi^2_m$}
\end{equation*}
So now we've got a chi-squared distribution, we can look up the value of $Q(m)$
in the tables.

\subsection{The Backshift Operator}
We define the \textbf{Backshift Operator} as:
\begin{equation*}
    \explain{B(x_t)}{The backshift operator} = x_{t-1}
\end{equation*}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=3aadc357-4876-4b61-bb69-4d6ac7347aeb}{Time Series Video 13}}

