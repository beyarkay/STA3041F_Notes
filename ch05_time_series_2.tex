\section{3041F: Time Series II Intro}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=41fa68b6-33c6-479e-b4a6-9909d6ec7edb}{Time Series Video 8}}
We'll be covering:
\begin{itemize}
    \item Stationarity, strict and weak variants.
    \item Autocorrelation: Measure the same variable but at different time points, and calculate the correlation between those timepoints
    \item Autocovariance
    \item Linear Time series models (moving average, autoregressive process, autoregressive moving average process)
\end{itemize}  

\subsection{Backshift Operator}
We'll (eventually) need the \textbf{Backshift Operator}, so define it as:
\begin{enumerate}
    \item \begin{equation*}
            B^iy_t = y_{t-i}
        \end{equation*}
    \item \begin{equation*}
            Bc = c
        \end{equation*}
    \item \begin{equation*}
            (B^i + B^j)y_t = B^iy_t + B^jy_t = y_{t-i} + y_{t-j}
        \end{equation*}
    \item \begin{equation*}
            B^iB^jy_t = B^jB^iy_t = B^iy_{t-j}= y_{t-j-i}   
        \end{equation*}
    \item Also, specially define the following:\begin{equation*}
            \theta(B) = 1 - \theta_1B - \theta_2B^2 - \dots - \theta_qB^q
    \end{equation*}
    \item For $|a| < 1$: \begin{equation*}
            (1 + aB + a^2B^2 + \dots )y_t = \frac{y_t}{1 - aB}
        \end{equation*}
    \item For $|a| > 1$: \begin{equation*}
            (1 + \frac{1}{aB} + \frac{1}{a^2B^2} + \dots )y_t = \frac{-aBy_t}{1
            - aB}
        \end{equation*}
    
\end{enumerate}

The backshift operator becomes \textbf{really} useful when used with linear
combinations of time series processes. For example, if we define $V_{t},W_{t},  Z_{t}$ as:
\begin{equation*}
    \begin{aligned}
        W_{t} &:= W_{t-1} - 0.16W_{t-2} + a_{t} \qquad \text{Where $a_t \iid N(0, \sigma^2)$}\\
        V_{t} &:= 0.8V_{t-1} + b_{t} \qquad \text{Where $b_t \iid N(0, \sigma^2)$}\\
        Z_{t} &:= W_{t} + V_{t} \\
    \end{aligned}
\end{equation*}
Then we can find $Z_{t} \sim ARMA(2, 1)$ by first expressing $W_{t}$ and
$V_{t}$ in terms of the backshift operator:
\begin{equation*}
    \begin{aligned}
        W_{t} &:= W_{t-1} - 0.16W_{t-2} + a_{t} \qquad \text{Where $a_t \iid N(0, \sigma^2)$}\\
        (1-B + 0.16B^{2})W_{t} &= a_{t} \\
        W_{t} &= \frac{a_{t}}{1-B + 0.16B^{2}} \\
        \text{And likewise for $V_{t}$ :}&\\
        V_{t} &:= 0.8V_{t-1} + b_{t} \qquad \text{Where $b_t \iid N(0, \sigma^2)$}\\
              &= 0.8BV_{t} + b_{t} \\
        (1-0.8B)V_{t} &= b_{t} \\
        V_{t} &= \frac{b_{t}}{1-0.8B} \\
    \end{aligned}
\end{equation*}
So now that we've gotten rid of all the $V_{t-1}, W_{t-1}, W_{t-2}$ terms, we
can rewrite our original expression $Z_{t} := W_{t} + V_{t}$ in terms of just
$Z_{t}$ and the backshift operator:
\begin{equation*}
    \begin{aligned}
        Z_{t} &:= W_{t} + V_{t} \\
        Z_{t} &= \frac{a_{t}}{1-B + 0.16B^{2}} + \frac{b_{t}}{1-0.8B} \\
    \end{aligned}
\end{equation*}
Which quickly yields to some manipulation to get a recognisable ARMA process:
\begin{equation*}
    \begin{aligned}
        Z_{t} &:= W_{t} + V_{t} \\
        Z_{t} &= \frac{a_{t}}{1-B + 0.16B^{2}} + \frac{b_{t}}{1-0.8B} \\
        Z_{t} &= \frac{a_{t}}{(1-0.8B)(1-0.2B)} + \frac{b_{t}}{1-0.8B} \\
        \text{Multiply by } & (1-0.8B)(1-0.2B) \\
        (1-0.8B)(1-0.2B)Z_{t} &= a_{t} + (1-0.2B)\cdot b_{t}  \\
        (1-B + 0.16B^{2})Z_{t} &= a_{t} + b_{t} - 0.2b_{t-1} \\
        Z_{t}-Z_{t-1} + 0.16Z_{t-2} &= a_{t} + b_{t} - 0.2b_{t-1} \\
        Z_{t} &= Z_{t-1} - 0.16Z_{t-2} + a_{t} + b_{t} - 0.2b_{t-1} \\
    \end{aligned}
\end{equation*}
The last line is an ARMA(2,1) process with $a_{t} + b_{t}$ forming essentially
one error term. So we can more easily manipulate combinations of time series by
(ab)using the backshift operator.


\section{Stationarity}
\subsection{Strict Stationarity}
Strict stationarity is a property of a time series.
\begin{itemize}
    \item A time series is strictly stationary iff for every offset parameter
        $m > 0$, all time points $t_1, \dots, t_n$, the joint distribution of
        $Z_{t_1}, \dots, Z_{t_n}$ is equal to that of $Z_{t_1 - m},
        \dots, Z_{t_n-m}$.
    \item We also require that if there are two points, $Z_t$ and  $Z_{t+m}$
        then  the distribution of the random vector  $\left( Z_t,
        Z_{t+m}\right)$ must be a function of the \textbf{lag} $|m|$.
    \item Given a timeline of a time series and an offset parameter $m$ 
    \item Consider Random Variables $Z_{t_1}, \dots, Z_{t_n}$, find the joint
        distribution of them all, then apply a timeshift by the offset
        parameter $m$: $Z_{t_{1-m}}, \dots, Z_{t_{n-m}}$.
    \item then if the joint distribution of the first set of Random variables
        is equal to that the joint distribution of the second set of random
        variables, then the time series is said to be \textbf{Strictly
        Stationary}.
\end{itemize}

\subsection{Autocovariance}
Define the \textbf{Autocovariance} as the covariance of the same random
variable but at different time points:
\begin{equation*}
    \begin{aligned}
        \mathbb{C}ov[Z_t, Z_{t+m}] &:= \gamma_{|m|} \\ 
                &= \mathbb{E}[Z_t \cdot Z_{t+m}] - \mathbb{E}[Z_t] \cdot
                \mathbb{E}[Z_{t+m}] \\
        &\text{Also note that:} \\
        \gamma_m = \gamma_{-m}
        \mathbb{V}[Z_t] &= \gamma_0 \\
    \end{aligned}
\end{equation*}
We can use $|m|$ because Covariance doesn't depend on the order of
the random variables given to it. 

Define the \textbf{Autocorrelation} as the autocovariance divided by the
variance of a time series. The autocorrelation (given the offset parameter
$m$) $\rho_m$ is defined by:
\begin{equation*}
    \begin{aligned}
        \rho_m &= \frac{\gamma_m}{\gamma_0} \\
        \rho_0 &= \frac{\gamma_0}{\gamma_0} = 1 \\
    \end{aligned}
\end{equation*}


\subsection{Weak Stationarity}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=c9dbfa7f-7ba8-4145-8037-26bdd235fd17}{Time Series Video 9}}

Weak stationarity is defined by orders (order 1, order 2, etc). So to have weak
stationarity of order $n$, the time series must have equal $n$-th moments.
For example, weak stationarity of order 1 requires a constant expectation.

Weak Stationarity of order 2 requires 
\begin{itemize}
    \item A constant expectation.
    \begin{equation*}
        \mathbb{E}[Z_t] = \mathbb{E}[Z_{t-m}]  \forall m
    \end{equation*}
        
    \item the autocovariance to only depend on the offset parameter $m$. For
        example, where $t$ and $s$ are valid indices to the time series $Z$:
    \begin{equation*}
        \mathbb{C}ov[Z_t, Z_{t-m}] = \mathbb{C}ov[Z_s, Z_{s-m}] 
    \end{equation*}
\end{itemize}
Therefore Weak Stationary of order 2 implies Weak Stationarity of order 1.

\subsection{Some examples of Weak stationarity}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=364aeb35-5750-4eb1-b844-93a239f8f81a}{Time Series Video 10}}
If we define a time series as $y_t = e_t - \frac{1}{2}e_{t-1}$ with all error
terms independant and $e_t \sim N(0, \sigma^2)$ then:

\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \mathbb{E}[e_t - \frac{1}{2}e_{t-1}] = 0 + 0 = 0 \\
        \gamma_0 &= \mathbb{E}[\left( e_t - \frac{1}{2}e_{t-1}\right)] - \mathbb{E}[y_t] \mathbb{E}[y_t]  \\
                 &= \dots \\
                 &= \sigma^2(1 + 0.25) + 0 \cdot 0 \\
                 &= 1.25 \sigma^2 \\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] \\
                 &= \mathbb{E}[\left( y_t \right) \left( y_{t-1}\right)] -
                 0 \cdot 0\\
                 &= \dots \\
                 &= -0.5\sigma^2 \\
        \gamma_k &= \mathbb{E}[y_t \cdot y_{t-k}]  - 0 \cdot 0\\
                 &= \mathbb{E}[\left( e_t - \frac{1}{2}e_{t-1} \right) \left(
                 e_{t-k} - \frac{1}{2}e_{t-1-k} \right)] \\
                 &= 0 - \frac{1}{2} \cdot 0 - \frac{1}{2} \cdot  0 +
                 \frac{1}{4} \cdot 0 \\
                 &= 0\\
    \end{aligned}
\end{equation*}

\subsection{More examples, introducing the autoregressive process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=929c1592-39cf-4d0d-9678-cdb3bf6f278b}{Time Series Video 11}}

We'll introduce the autoregressive (AR) process as having parameter $|\phi| <
1$ and being a time series like:
\begin{equation*}
    y_t = \phi y_{t-1} + e_t 
\end{equation*}
Which implies that the expectation can be calculated as:
\begin{equation*}
    \mathbb{E}[y_t] = \dots = \phi \mathbb{E}[y_{t-1}] 
\end{equation*}
So now we've got a self-referencial statement where the expectation at time
$t$ depends on the expectation at time $t-1$. This can collapse to become:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \lim_{h\to\infty} \phi^h \mathbb{E}[y_{t-h}]  \\
            &= 0 \cdot \mathbb{E}[y_{t-h}]  \\
            &= 0 \\
    \end{aligned}
\end{equation*}
And the variance will similarily be self-referencial, and collapse to give us:
\begin{equation*}
    \mathbb{V}[y_t] = \dots = \phi^{2h} \mathbb{V}[y_{t-h}] + \sigma^2
    \sum_{i=0}^{h-1} \phi^{2i}
\end{equation*}
And so taking the limit as $h\to\infty$:
 \begin{equation*}
     \begin{aligned}
         \mathbb{V}[y_t] &= \sigma^2 \sum_{i=0}^{h-1} \phi^{2i}\\
                         &= \frac{\sigma^2}{1 - \phi^2} \\
     \end{aligned}
\end{equation*}

\section{Autocorrelation}
Given a certain time series like $X_t = (x_1, \dots, x_n)$ we can
calculate the autocorrelation for offset $m$ like so:
\begin{equation*}
    \hat{\rho}_m = \mathbb{C}orr[X_t, X_{t-m}] = \frac{
        \sum_{t=m+1}^{n} \left( X_t - \bar{X} \right) \left( X_{t-m} - \bar{X} \right)
        }{
        \sum_{t=1}^{n} \left( X_t - \bar{X}\right)^2
        }
\end{equation*}
A \textbf{Correlogram} is a plot with $m$ on the x axis and the $m$-th lagged
autocorrelations on the y axis.

\section{Significance testing of Autocorrelation}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=7322200d-179b-4704-970c-e17c8cc40f66}{Time Series Video 12}}
Note that when you calculate autocorrelation with an offset of $m$, you'll lose
exactly $m$ data points. For example, in calculating the autocorrelation with
$m=1$, there is no $Z_{t_0}$ data point to match up with the  $Z_{t_1}$ data
point.

Today we look at the statistical significance of each of the $m$
autocorrelations. If the underlying process is a moving average process, then
the number of statistically significant autocorrelations is equal to the order
of that moving average process.

Recall that the $m$-th autocorrelation is defined as:
\begin{equation*}
    \hat{\rho}_m = \frac{
        \sum_{t=m+1}^{n} \left( X_t - \bar{X} \right) \left( X_{t-m} - \bar{X} \right)
        }{
        \sum_{t=1}^{n} \left( X_t - \bar{X}\right)^2
        }
\end{equation*}
Now we can form a standard normal random variable by subtracting the mean and
dividing by the sample variation. We have to find the distribution of
$\hat{\rho}_m$ numerically, by simulating the histogram many many times (No
easy analytical solution exists). After doing this, we find:
\begin{equation*}
    \hat{\rho}_m \sim N(0, \frac{1}{n})
\end{equation*}
And so we can construct a standard normal like so:
\begin{equation*}
    Z = \frac{\hat{\rho}_m - 0}{\frac{1}{\sqrt{n}}}
\end{equation*}

\subsection{Hypothesis tests with Autocorrelation}
We know that $\hat{\rho}_m \sim N(0, \frac{1}{n})$ (as $n\to\infty$).  Thus if
$|\hat{\rho}_m| > \frac{t_{\alpha/2}}{\sqrt{n}}$ we would reject $H_0: \rho_m =
0$ at the  $\alpha$ significance level (where $t_{\alpha/2}$ is the usual value
from student's t-distribution).

The \textbf{Bartlett's Test} is the same as above, except we reject $H_0$ if
$|\hat{\rho}_m| > \frac{2}{\sqrt{n}}$ because fuck accuracy.

The \textbf{Portmanteau test} allows us to also test more complicated
hypotheses such as:
\begin{equation*}
    \begin{aligned}
        H_0: \hat{\rho}_i &= 0 \quad \forall i \in \{1, 2, \dots, k\} \\
        H_1: \hat{\rho}_i &\ne 0 \quad \text{For at least one} i \in \{1, 2, \dots, k\} \\
    \end{aligned}
\end{equation*}
Recalling that the sum of squared independent standard normals is
chi-square distributed, we define the Portmanteau test for offset $m$ as:
\begin{equation*}
    \explain{Q(m) = n \sum_{i=1}^{m} \hat{\rho}_i^2}{$\sim \chi^2_m$}
\end{equation*}
So now we've got a chi-squared distribution, we can look up the value of $Q(m)$
in the tables.

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=3aadc357-4876-4b61-bb69-4d6ac7347aeb}{Time Series Video 13}}
\section{Linear Models for Time Series}
\subsection{White Noise Process}
This process is defined as only having an error term $e_t$ like:
 \begin{equation*}
     y_t = e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
So we can calculate the moments as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= 0 \\
        \mathbb{V}[y_t] &= \sigma^2 \\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = 0 \\
        \gamma_k &= \dots = 0 \\
    \end{aligned}
\end{equation*}

\subsection{The Random Walk Process}
This is defined as being dependant on the previous time step:
\begin{equation*}
    \begin{aligned}
        y_t &= y_{t-1} + e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$} \\
            &\text{And we (usually) assume $y_0$ as:} \\
        y_0 &= 0 \\
    \end{aligned}
\end{equation*}
Note that the errors are cumulative:
\begin{equation*}
    \begin{aligned}
        y_t &= y_0 + \sum_{i=1}^{t} e_i \\
            &= \sum_{i=1}^{t} e_i \\
    \end{aligned}
\end{equation*}
Recall that if $X_1 \indep X_2$ then:
\begin{equation*}
        \mathbb{E}[X_1 \cdot X_2] = \mathbb{E}[X_1] \cdot \mathbb{E}[X_2]
\end{equation*}

So the moments are given as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \dots = y_0 = 0  \\
        \mathbb{V}[y_t] &= \dots = t\sigma^2\\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = (t-1) \sigma^2 \\
                 &= \text{Note in general, if we have $s < t$:} \\
        \mathbb{C}ov[y_t, y_s] &= s \sigma^2 \\
        \gamma_k &= \dots = 0 \\
    \end{aligned}
\end{equation*}


\subsection{The Random Walk Process with Drift}
This is defined as being dependant on the previous time step, in addition to
some accumulative term:
\begin{equation*}
    \begin{aligned}
        y_t &= y_{t-1} + \mu + e_t \qquad \text{Where $e_t \iid N(0, \sigma^2)$} \\
            &\text{And we (usually) assume $y_0$ as:} \\
        y_0 &= 0 \\
    \end{aligned}
\end{equation*}

So the moments are given as:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[y_t] &= \dots = y_0 + t\mu  \\
        \mathbb{V}[y_t] &= \dots = t\sigma^2\\
        \gamma_1 &= \mathbb{C}ov[y_t, y_{t-1}] = \dots = (t-1) \sigma^2 \\
                 &= \text{Note in general, if we have $s < t$:} \\
        \mathbb{C}ov[y_t, y_s] &= s \sigma^2 \\
    \end{aligned}
\end{equation*}
Define the \textbf{difference operator} as having symbol $\nabla$ and being
like:
\begin{equation*}
    \nabla y_t := y_t - y_{t-1} = \explain{\mu + e_t}{For a random walk with
    drift}
\end{equation*}
Often the difference operator can help create a stationary process from a non
stationary one.

\section{First Order Moving Average process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=757a8720-2693-47b1-9068-e6352a7edd93}{Time Series Video 14}}
We'll start talking about first order, then second order, and then we'll
abstract to talking about the $q$-th order moving average process. For now we
talk about the first order moving average process and define it as:
\begin{equation*}
    Z_t = e_t - \theta e_{t-1} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
Where $\theta$ is some weight that we can choose. Note that the sign between
the two terms is irrelevant, as we can choose $\theta$ to counteract whatever
sign we choose.

We can then find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \gamma_0 &= \mathbb{V}[Z_t] = \sigma^2(1 + \theta^2) \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= 
           \begin{cases}
               \sigma^2(1 + \theta^2) & \text{For $k=0$} \\
                   -\theta\sigma^2 & \text{For $k=1$} \\
                   0  & \text{For $k>1$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
                1 & \text{$k = 0$}\\
                -\frac{\theta}{1 + \theta^2} & \text{$k = 1$}\\
                0  & \text{For $k>1$} \\
            \end{cases}
    \end{aligned}
\end{equation*}

\section{Second Order Moving Average Process}
\begin{equation*}
    Z_t = e_t - \theta_1 e_{t-1} - \theta_2 e_{t-2} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
We can find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= \begin{cases}
               \sigma^2 (1 + \theta_1^2 + \theta_2^2) & \text{For $k=0$} \\
                   \sigma^2\theta_1(\theta_2 - 1) & \text{For $k=1$} \\
                   -\theta_2\sigma^2 & \text{For $k=2$} \\
                   0  & \text{For $k>2$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
            1  \quad &k= 0 \\
            \frac{\theta_1(\theta_2 - 1)}{1 + \theta_1^2 + \theta_2^2} & k=1 \\
            \frac{-\theta_2}{1 + \theta_1^2 + \theta^2_2} & k=2 \\
            0 & k>2 \\
            \end{cases}
    \end{aligned}
\end{equation*}

\section{Q-th Order Moving Average Process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=372248c4-2710-4c42-a7df-ff73b1eabcf1}{Time Series Video 15}}
\begin{equation*}
    Z_t = e_t - \sum_{i=1}^{q} \theta_i e_{t-i} \qquad \text{Where $e_t \iid
    N(0, \sigma^2)$}
\end{equation*}

We can find the moments to be:
\begin{equation*}
    \begin{aligned}
       \mathbb{E}[Z_t] & = 0 \\
       \mathbb{V}[Z_t] & = \sigma^2 + \sigma^2 \sum_{i=1}^{q} \theta^2_i \\
       \gamma_k &= \mathbb{E}[Z_t \cdot Z_{t-k}] \\
           &= \begin{cases}
               -\sigma^2\theta_k + \sigma^2 \sum_{i=1}^{q}
               \theta_{k+i}\theta_i &  \text{For $1 \le k \le q$} \\
               0 & \text{For $k > q$} \\
           \end{cases} \\
        \rho_k &= \begin{cases}
            \frac{
                -\theta_k + \sum_{i=1}^{q} \theta_{k+i}\theta_i
            }{
                1 + \sum_{i=1}^{q} \theta_i^2
            } &  \text{For $1 \le k \le q$} \\
               0 & \text{For $k > q$} \\
            \end{cases}
    \end{aligned}
\end{equation*}

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=f3d183b3-57bf-45c4-8899-d32d7b58616f}{Time Series Video 16}}
\section{General Linear Process}
\textbf{General Linear Process}: A weighted infinite sum of error terms, where
we call the ith weight $\psi_i$.
\begin{equation*}
    \begin{aligned}
        \sum_{i=1}^{\infty} \psi_i^2 &< \infty \\
        |\psi_i| &< 1 \\
        \psi_0 &= 1 \\
        Z_t &= \sum_{i=0}^{\infty} \psi_i \cdot e_{t-i} \\
    \end{aligned}
\end{equation*}
For example, we can set $\psi_i = (\frac{1}{2})^2$. This obeys the
requirements.

With a question like \textbf{Identify this process}, you just need to say if
it's a General Linear Process, a moving average process, etc.

With the example linear process like: $\psi_i = \phi^i$ and with $\phi_i \in
(0, 1)$ we've got a fully define process given by:

\begin{equation*}
    \begin{aligned}
        Z_t &= \sum_{i=0}^{\infty} \phi^i e_{t-i} \\
        \gamma_0 &= \mathbb{E}[Z_t^2]\\
                 &= \sigma^2 \sum_{i=0}^{\infty} \phi^{2i}\\
                 &= \frac{\sigma^2}{1 - \phi^2}
        \gamma_k &= \frac{\phi^k \sigma^2}{1 - \phi^2} \qquad \forall k=0,1,.. \\
        \rho_k &= \phi^k \qquad \forall k=0,1,.. \\
    \end{aligned}
\end{equation*}

An example of calculating the autocovariance $\gamma_k$ :
\begin{equation*}
    \begin{aligned}
        \gamma_k &= \mathbb{C}ov[Z_t, Z_{t+k}] = \mathbb{E}[Z_t \cdot Z_{t_k}] \\
                 &= \mathbb{E}[\left( \sum_{i=0}^{\infty} \phi^i e_{t-i}\right)
                 \left(\sum_{j=0}^{\infty} \phi^j e_{t+k-j} \right)] \\
                 &\text{let } t - i = t + k - j \iff j = i+k\\
        \gamma_k = \mathbb{E}[\left( \sum_{i=0}^{\infty} \phi^i e_{t-i}\right)
                \left(\sum_{i=0}^{\infty} \phi^{i+k} e_{t-i} \right)] \\
            &= \phi^k \mathbb{E}[\left( \sum_{i=0}^{\infty} \phi^i
                 e_{t-i}\right)^2] \\
            &= \frac{\phi^k \sigma^2}{1 - \phi^2} \\
    \end{aligned}
\end{equation*}
Note that general linear processes are weakly stationary of order 2 since
neither the mean nor the covariance depends on time $t$.


\section{Infinite Moving Average Process}
Very similar to the above General Linear Process:
\begin{equation*}
    Z_t = \sum_{i=0}^{\infty} \omega_i e_{t-i} \qquad \omega_0 = 1
\end{equation*}

\begin{equation*}
    \begin{aligned}
        \mathbb{E}[Z_t] &= 0 \\
        \gamma_0 &= \lim_{T\to\infty} \sigma^2 \sum_{i=0}^{T} \omega_i^2 \\
        \gamma_k &= \lim_{T\to\infty} \sigma^2 \sum_{i=0}^{T} \omega_i
        \omega_{k+i} \\
        \rho_k &= \lim_{T\to\infty} \frac{\sum_{i=0}^{T} \omega_i
        \omega_{k+i}}{sum_{i=0}^{T} \omega_i^2} \\
    \end{aligned}
\end{equation*}


\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=ea9eba37-1039-4ca0-a4ab-7178816a8345}{Time Series Video 17}}
Have previously looked at the moving average process.
Will now be looking at forecasts for moving average processes
\subsection{Forecasts for Moving Average Processes}
With the moving average process defined by:
\begin{equation*}
    Z_t = e_t - \phi e_{t-1} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
And now lets define the forecast value as $\hat{Z}_{t+1}$ and we set it equal
to be:
 \begin{equation*}
     \begin{aligned}
         \hat{Z}_{t+1} &= \mathbb{E}[Z_{t+1} | \explain{Z_t, Z_{t-1} \dots
         Z_1}{Often called just $I_t$, the information at  $t$}]  \\
                       &= \mathbb{E}[e_{t+1} - \phi e_t | I_t]  \\
                       &= \explain{\mathbb{E}[e_{t+1} | I_t]}{$e_{t-1}$ is
                       still a random variable} - \phi \explain{\mathbb{E}[e_t
                   | I_t]}{$e_t$ has a measured value}  \\
                       &= 0 - \phi e_t
                       &= -\phi e_t
     \end{aligned}
\end{equation*}
So the forecast for a moving average process is $ \hat{Z}_{t+1} = -\phi e_t$.

We often also want to know \textbf{the margin of error}, so we define the one
step ahead forecast error $ \hat{e}(1)$ as:
\begin{equation*}
    \begin{aligned}
        \hat{e}(1) &= Z_{t+1} - \hat{Z}_{t+1} \\
                   &= (e_{t+1} - \phi e_t) - (- \phi e_t) \\
                   &= e+{t+1}
                   &\text{Note that we can also calculate:}\\
        \mathbb{V}[ \hat{e}(1)] &= \sigma^2 \\
    \end{aligned}
\end{equation*}

Now, for a \textbf{MA(q) model} , we have:
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+l} &= - \sum_{i=l}^{q} \phi_i e_{t-i+l} \qquad \forall 1 
        \le l \le q \\
        \hat{e}(l) &= Z_{t+l} - \hat{Z}_{t+l} \\
            \mathbb{V}[ \hat{e}(l)] &= \begin{cases}
            \sigma^2 & \text{for $l=1$}\\
            \sigma^2 \left( 1 + \sum_{i=1}^{l-1} \phi_i^2 \right)& \text{for $
            2 \le l \le q$}\\
            \sigma^2 \left( 1 + \sum_{i=1}^{q} \phi_i^2 \right)& \text{for $l >
            q$}\\
        \end{cases}
    \end{aligned}
\end{equation*}


\section{Autoregressive Process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?id=35d8e9d3-94b8-4149-90d1-e85a0c8c11da&ltimode=true}{Time Series Video 18}}
A first order autoregressive process (AR(1))can be defined as a weighted linear
sum of the previous value in the time series $Z_{t-1}$ and an error term
$e_{t}$:
\begin{equation*}
    Z_t = \phi Z_{t - 1} + e_{t} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
Since $Z_{t}$ depends on $Z_{t-1}$, this can be expanded into an infinite
moving average process (assuming $|\phi| < 1$:
\begin{equation*}
    Z_{t} = \sum_{i=0}^{\infty} \phi^{i} e_{t-1}
\end{equation*}
So some properties are given by:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[Z_{t}] &= 0 \\
        \mathbb{V}[Z_{t}] &= \frac{\sigma^{2}}{1 - \phi^{2}}\\
        \gamma_{k} &= \frac{\phi^{k}\sigma^{2}}{1-\phi^{2}} = \phi^{k}
        \gamma_0  \\
        \rho_{k} &= \phi^{k} \\
    \end{aligned}
\end{equation*}

We can also write the AR(1) process as:
\begin{equation*}
    \begin{aligned}
        e_{t} &= Z_{t} - \phi Z_{t-1} = (1 - \phi B) Z_{t}\\ 
        &= \phi(B)Z_{t} \\ 
    \end{aligned}
\end{equation*}


\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?id=3a8d235f-b875-4370-92aa-25d86f869e8d&ltimode=true}{Time Series Video 19}}
Now we'll write the AR(2) process as:
\begin{equation*}
    \begin{aligned}
        Z_{t} &= \phi_{1}Z_{t-1} + \phi_{2}Z_{t-2} + e_t \qquad \text{Where $e_t
        \iid N(0, \sigma^2)$} \\
              &\text{Or alternatively:}\\
            (1 - \phi_{1}B - \phi_{2}B^{2})Z_{t} &= e_{t} \\
    \end{aligned}
\end{equation*}
And if we assume the time series is weakly stationary of order 2, and that
$\sum_{i=1}^{2} \phi_{i} \ne 1$: 
(\textbf{WS(2)}) then:
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[Z_{t}] &= 0 \\
        \gamma_{0} &= \left( \frac{1 - \phi_{2}}{1 + \phi_{2}} \right) \frac{\sigma^{2}}{1 - \phi^{2}_{2} - \phi_{1}^{2}} \\
        \gamma_{k} &= \phi_{1}\gamma_{k-1} + \phi_{2}\gamma_{k-2} \\
        \rho_{k} &= \phi_{1}\rho_{k-1} + \phi_{2}\rho_{k-2} \\
                          &\text{Where $\rho_{-1} = \rho_{1}$ and $\rho_{0}=1$.}\\
   \end{aligned}
\end{equation*}
The above equations for $\gamma_{k}$ and $\rho_{k}$ are called the
\textbf{Yule-Walker} equations.

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?id=61de3aff-2f8f-4535-973e-f7539d887186&ltimode=true}{Time Series Video 20}}
\subsection{The Characteristic Equation}
Taking the formula for a AR(2) process, we can write it as:
\begin{equation*}
    \begin{aligned}
        Z_{t} &= \phi_{1}Z_{t-1} + \phi_{2}Z_{t-2} + e_t \\
        e_{t} &= Z_{t}\left( \explain{1 - \phi_{1}B - \phi_{2}B^{2}}{Equate
        this to zero} \right) \\
                1 - \phi_{1}B - \phi_{2}B^{2} &= 0 \\
                \text{Let $x = \frac{1}{B}$} &\\
                x^{2} - \phi_{1}x - \phi_{2} &= 0 \\
                x &= \frac{\phi_{1} \pm \sqrt{\phi^{2}_{1} + 4\phi_{2}}}{2}
    \end{aligned}
\end{equation*}

In general, an \textbf{AR(q) process is weakly stationary of order 2 iff} the
characteristic roots lie inside the unit circle in the complex plane, ie:
\begin{equation*}
    \begin{aligned}
        \phi_{1} + \phi_{2} &< 1 \\
        \phi_{2} - \phi_{1} &< 1 \\
        |\phi_{2}| &< 1 \\
    \end{aligned}
\end{equation*}

\subsection{P-th order Autoregressive Process}
The p-th order autoregressive process AR(p) can be written as a linear
combination of the previous p time points, plus some random error.
\begin{equation*}
    \begin{aligned}
        Z_{t} = e_{t} + \sum_{i=1}^{p} \phi_{i}Z_{t-i} \qquad \text{Where $e_t
        \iid N(0, \sigma^2)$}
    \end{aligned}
\end{equation*}

Then the lag-k autocovariance $\gamma_{k}$ can be found by:
\begin{equation*}
    \begin{aligned}
        \gamma_{k} &= \mathbb{E}[Z_{t} \cdot Z_{t-k}] \\
        Z_{t} \cdot Z_{t-k} &= Z_{t-k} \cdot e_{t} + Z_{t-k}\cdot\sum_{i=1}^{p}
        \phi_{i}Z_{t-i} \\
        \gamma_{k} &= \mathbb{E}[Z_{t-k} \cdot e_{t}] + Z_{t-k}\cdot\sum_{i=1}^{p}
        \phi_{i}Z_{t-i}] \\
        \gamma_{k} &= \mathbb{E}[Z_{t-k} \cdot e_{t}]
        +\mathbb{E}\left[Z_{t-k}\cdot\sum_{i=1}^{p} \phi_{i}Z_{t-i}\right] \\
    \end{aligned}
\end{equation*}
And now we can label the following equations as the Yule-Walker Equations:
\begin{equation*}
    \begin{aligned}
    \gamma_{k} &= 0 + \sum_{i=1}^{p} \phi_{i}\gamma_{k-i} \\
    \rho_{k} &= \sum_{i=1}^{p} \phi_{i}\phi_{k-i} \\
    \end{aligned}
\end{equation*}

\subsection{Finding the Order of an Autocorrelation Model}
The aim is to use data we've measured to find the best value for $p$, and to
evaluate how good this best value is.
We also define 
\begin{itemize}
    \item \textbf{ACF}: Autocorrelation Function and 
    \item \textbf{PACF}: Partial Autocorrelation Function
\end{itemize}

And then the lag-i partial autocorrelation is defined as $\phi_{ii}$ with the
double subscript to differentiate from the coefficients of the autoregressive
process, $\phi_{i}$.

The partial autocorrelation between $Z_{t}$ and $Z_{t-k}$ is the correlation
between $Z_{t}$ and $Z_{t-k}$ but with all the in between correlations from
$Z_{t-1}, \dots Z_{t-k-1}$ removed. This is done by subtracting a linear
combination of all the in between time series points, with the coefficients are
assigned as $\alpha_{1}, \dots , \alpha_{k-1}$. To make this explicit:
\begin{equation*}
    \phi_{ii} = \mathbb{C}orr\left[Z_{t} - \sum_{i=1}^{k-1} \alpha_{i}
    Z_{t-i},\quad Z_{t-k} -
\sum_{i=1}^{k-1} \alpha_{i} Z_{t-i} \right]
\end{equation*}

\subsubsection{Mean Square Error estimation of $\phi_{ii}$}
To solve for $\phi_{ii}$, note how we can use FOIL when calculating
covariance:
\begin{equation*}
    \mathbb{C}ov\left[A - B, C - D\right] = \mathbb{C}ov\left[A, C\right] -
    \mathbb{C}ov\left[A, D\right] - \mathbb{C}ov\left[B, C\right] +
    \mathbb{C}ov\left[B, D\right] 
\end{equation*}
But before we can use the above formula, we need estimates for $\alpha_{i}$.

We can define the MSE as the minimum value with respect to $ \alpha$ of:
\begin{equation*}
    \begin{aligned}
        \text{MSE} &= \mathbb{E}\left[(Z_{t} - \alpha Z_{t-1})^{2}\right] \\
        &= \mathbb{E}\left[Z_{t}^{2} - 2\alpha Z_{t}Z_{t-1} +
        \alpha^{2}Z_{t-1}^{2}\right]  \\
        &= \gamma_{0} - 2\alpha\gamma_{1} + \alpha^{2}\gamma_{0} \\
        &\text{Taking the derivative with respect to $\alpha$:}\\
        \frac{d\text{MSE}}{d\alpha} &= 0 \\
        -2\gamma_{1} + 2\alpha\gamma_{0} &= 0 \\
        \alpha &= \frac{2\gamma_{1}}{2\gamma_{0}} \\
               &= \rho_{1} \\
    \end{aligned}
\end{equation*}
So we now can calculate $\phi_{22}$ as:
\begin{equation*}
    \begin{aligned}
        \phi_{22} &= \mathbb{C}orr\left[Z_{t} - \alpha Z_{t-1},\quad Z_{t-2} -
        \alpha Z_{t-1} \right] \\
                  &= \frac{\gamma_{2} - \rho_{1} \gamma_{1} -
                  \rho_{1}\gamma_{1} + \rho_{2}^{2}\gamma_{0}}{
              \mathbb{V}\left[Z_{t} - \rho_{1} Z_{t-1}\right] } \\
                  &= \frac{\gamma_{2} - 2\rho_{1}\gamma_{1} +
                  \rho_{1}^{2}\gamma_{0}}{\gamma_{0} - 2\rho_{1}\gamma_{1} +
              \rho_{1}^{2}\gamma_{0}} \\
                  &\text{Note $\rho_{1} = \phi$, and $\rho_{2} = \phi \rho_{1}$}
                  &= \frac{\rho_{1} - 2 \rho_{1}^{2} + \rho_{1}^{2}}{1 - 2
                  \rho_{1}^{2} + \rho_{1}^{2}} \\
                  &= \frac{\phi^{2} - 2 \phi^{2} + \phi^{2}}{1 - 2\phi^{2} +
                  \phi^{2}} \\
                  &= 0 \\
    \end{aligned}
\end{equation*}
So the lag-2 partial autocorrelation is equal to zero. This is because the
autoregressive process is of order 1, \textbf{AR(1)} so any lag-n partial
autocorrelation with n greater than 1 will be zero.
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=cc9af55f-c9a6-4603-ba43-798ad514e018}{Time Series Video 21}}
31 minutes
Today:
\begin{enumerate}
    \item calculating $\phi_{33}$
    \item Forecasting for an AR Process
    \item How to identify an AR Process via EDA with measured data.
\end{enumerate}
Recall that the number of statistically significant partial autocorrelations is
equal to the order of the AR process. Then the parameters of the AR process can
be estimated via least squares.

\subsection{Calculating lag-3 partial autocorrelations of a AR(1) process}

With a model like:
\begin{equation*}
    Z_{t} = \phi Z_{t-1} + e_{t} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
\textbf{Theorem:} Any partial autocorrelation of order greater than the order
of the AR process will be zero

So $\phi_{11}$ is the only non zero of $\phi_{11}, \phi_{22}, \dots $

Where $\alpha_{i}$ and $\beta_{i}$ are unknown constants of the model that we
will estimate via the values that minimise the mean squared error.
\begin{equation*}
    \begin{aligned}
        \phi_{33} &= \mathbb{C}orr\left[Z_{t} - \alpha_{1}Z_{t-1} -
        \alpha_{2}Z_{t-2}, Z_{t-3} - \beta_{1}Z_{t-1} - \beta_{2}Z_{t-2}\right] \\
    \end{aligned}
\end{equation*}

We define the MSE as the minimum value with respect to $ \alpha$ of:
\begin{equation*}
    \begin{aligned}
        \text{MSE} &= \min_{\alpha}\left[\mathbb{E}\left[(Z_{t} - \alpha_{1}
        Z_{t-1} - \alpha_{2} Z_{t-2})^{2}\right] \right] \\
                   &= \text{Expand the trinomial...} \\
                   &= \min_{\alpha} \left[ \gamma_{0} +
                   \alpha_{1}^{2}\gamma_{0} + \alpha_{2}^{2}\gamma_{0} -
               2\alpha_{1}\gamma_{1} - 2\alpha_{2}\gamma_{2} +
           2\alpha_{1}\alpha_{2}\gamma_{1} \right] \\
                   & \text{Setting the derivative to zero} \\
                \frac{d \text{MSE}}{d \alpha_{1}} &= 0 \\
                    &= 2 \alpha_{1}\gamma_{0} - 2 \gamma_{1} + 2\alpha_{2}\gamma_{1} \\
                    &= 2\alpha_{1}\gamma_{0} + \gamma_{1}\left( -2 + 2\alpha_{2} \right) \\
                \frac{d \text{MSE}}{d \alpha_{2}} &= 0 \\
                    &= 2\alpha_{2}\gamma_{0} - 2\gamma_{2} + 2 \alpha_{1}\gamma_{1} \\
                    & \text{Use the two equations to simultaneously find values for $\alpha_{1}$ and $\alpha_{2}$} \\
                    & \text{Then we can find $\phi_{33} as:$} \\
                    \phi_{33} &= \mathbb{C}orr\left[Z_{t} - \alpha_{1}Z_{t-1} -
                    \alpha_{2}Z_{t-2}, Z_{t-3} - \beta_{1}Z_{t-1} -
                \beta_{2}Z_{t-2}\right] \\
    \end{aligned}
\end{equation*}
Now note that we can calculate correlation using FOIL like:
\begin{equation*}
    \begin{aligned}
        \mathbb{C}orr\left[a_{1}X + b_{1}Y, a_{2}Z + b_{2}T\right] &= \frac{
\mathbb{C}ov\left[a_{1}X + b_{1}Y, a_{2}Z + b_{2}T\right] 
        }{
\mathbb{V}\left[a_{1}X + b_{1}Y\right] 
    } \\
    &= \frac{a_{1}a_{2} \mathbb{C}ov\left[X, Z\right] + a_{1}b_{2}
    \mathbb{C}ov\left[X, T\right] +b_{1}a_{2} \mathbb{C}ov\left[Y, Z\right] +
b_{1}b_{2} \mathbb{C}ov\left[Y, T\right] }{\gamma_{0}} \\
    \end{aligned}
\end{equation*}
for the second line, we assume that the two variables $a_{1}X + b_{1}Y$
and $a_{2}Z + b_{2}T$ have equal variance (ie the process is \textbf{WS(2)}).

\subsection{Using Real Data to make a model and then forecast}
If you have measured data points $z_{1}, \dots z_{n}$. Via some glossed over
EDA, God tells us that this data comes from an autoregressive process of order
3. So we can write a model for the data like:
\begin{equation*}
    Z_{t} = c + \phi_{1}Z_{t-1} + \phi_{2}Z_{t-2} + \phi_{3}Z_{t-3} + e_{t}
\end{equation*}
Now to find values for the parameters, we'll do regression analysis and the
Ordinary Least Squares analysis. 

Note that because the model dictates the data points are not completely
independent of each other, we require the error term to be a white noise
process for the OLS analysis to hold mathematically. We also won't be able to
use the first and last few data points as the model requires the $(t-3)$th data
point to estimate the $t$-th data point.


\subsection{Checking the validity of a AR model}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=150758dd-b263-45f7-a607-a4833c17b89e}{Time Series Video 22}}
We'll be looking at the residuals and checking our assumption that they come
from a white noise process. We'll calculate the autocorrelations of these white
noise processes and assert that there are no statistically significant
autocorrelations.

\subsection{Forecasting with an AR process}
With the model given by:
\begin{equation*}
    Z_{t} = \phi Z_{t-1} + e_{t} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}

The \textbf{K-step ahead forecast} is defined as:
\begin{equation*}
    \hat{Z}_{t+k} = \mathbb{E}\left[Z_{t+k} | I_{t}\right] 
\end{equation*}
\subsubsection{1-step ahead forecast}
For example, the 1-step ahead forecast is given by:
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+1} &= \mathbb{E}\left[Z_{t+1} | Z_{t}, Z_{t-1}, \dots
        Z_{1}\right]  \\
                      &\text{So now we'll calculate $Z_{t+1}$ }\\
            Z_{t+1} &= \phi Z_{t} + e_{t+1} \\
        \hat{Z}_{t+1} &= \mathbb{E} \left[  \phi Z_{t} + e_{t+1} | I_{t}\right] \\
                      &= \phi Z_{t} +
                      \explain{\mathbb{E}\left[e_{t+1}\right]}{equal to zero}  \\
                      &= \phi Z_{t} \\
    \end{aligned}
\end{equation*}
So now that we've got  $ \hat{Z}_{t+1}$, lets find the variance of it. We'll
define the \textbf{one-step ahead forecast error} $ \hat{e}(1)$ as:
\begin{equation*}
    \begin{aligned}
        \hat{e}(1) &= Z_{t+1} - \hat{Z}_{t+1} \\
                   &= \phi Z_{t} + e_{t+1} - \phi Z_{t} \\
                   &= e_{t+1} \\
    \end{aligned}
\end{equation*}
And we can calculate the variance of the one-step ahead forecast error as:
\begin{equation*}
    \begin{aligned}
        \mathbb{V}\left[ \hat{e}(1)\right] &= \mathbb{V}\left[ e_{t+1} \right]
        \\
                                           &= \sigma^{2} \\
    \end{aligned}
\end{equation*}

\subsubsection{2-step ahead forecast}
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+2} &= \mathbb{E}\left[Z_{t+1} | I_{r}\right]  \\
                      &= \mathbb{E}\left[\phi Z_{t+1} + e_{t+2} | I_{t}\right]  \\
                      &= \phi \explain{\mathbb{E}\left[Z_{t+1} |
                      I_{t}\right]}{1-step ahead forecast} +
                      \explain{\mathbb{E}\left[e_{t+1} | I_{t}\right] }{ R.V.
                      is zero} \\
                      &= \phi Z_{t+1} + 0 \\
    \end{aligned}
\end{equation*}
So the two step ahead forecast is just the one-step ahead forecast times
$\phi$.
The error of the forecast, and the variance thereof is given by:
\begin{equation*}
    \begin{aligned}
        \hat{e}(2) &= Z_{t+2} - \hat{Z}_{t+2}\\
                   &= \phi Z_{t+1} + e_{t+2} - \phi \hat{Z}_{t+1} \\
                   &= \phi \left( Z_{t+1} - \hat{Z}_{t+1} \right) + e_{t+2} \\
                   &= \phi \hat{e}(1) + e_{t+2} \\
        \mathbb{V}\left[\hat{e}(2)\right]  &= \mathbb{V}\left[\phi \hat{e}(1) + e_{t+2}\right]  \\
                                           &= \phi^{2} \sigma^{2} + \sigma^{2}\\
                                           &= \sigma^{2}(1 + \phi^{2}) \\
    \end{aligned}
\end{equation*}
So the variance of the 1-step forecast error was $\sigma^{2}$ and then for the
2-step forecast error, the variance grew to $\sigma^{2}(1 + \phi^{2})$ (where
$0 < \phi^{2} \le \phi < 1$
       
\subsection{Sections in the notes that will not be covered}
\begin{itemize}
    \item The unit root process.
\end{itemize}

\subsection{AR(2) Process forecasting}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=71504881-c12b-4405-94ab-148592068f9e}{Time Series Video 23}}
We'll be doing 1, 2, and 3 step forecasting for a AR(2) process given by:
\begin{equation*}
    Z_{t} = \phi_{1} Z_{t-1} + \phi_{2} Z_{t-2} + e_{t} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
So we'll calculate the 1 step forecast by:
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+1} &= \mathbb{E}\left[Z_{t+1} | I_{t}\right] \\
                      &= \mathbb{E}\left[ \phi_{1} Z_{t-1} + \phi_{2} Z_{t-2} +
                      e_{t} | I_{t} \right]  \\
                      &= \phi_{1}Z_{t} + \phi_{2}Z_{t-1} + 0 \\
        \hat{e}(1) &= Z_{t+1} - \hat{Z}_{t+1} = e_{t+1} \\
        \mathbb{V}\left[\hat{e}(1)\right]  &= \sigma^{2}\\
    \end{aligned}
\end{equation*}
And the 2-step ahead forecast as:
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+2} &= \mathbb{E}\left[Z_{t+2} | I_{t}\right] \\
                      &= \mathbb{E}\left[ \phi_{1} Z_{t+1} + \phi_{2} Z_{t} +
                      e_{t+2} | I_{t} \right]  \\
                      &= \phi_{1} \hat{Z}_{t+1} + \phi_{2} Z_{t} + 0 \\
        \hat{e}(2) &= Z_{t+2} - \hat{Z}_{t+2} \\
                   &= \phi_{1} \hat{e}(1) + e_{t+2} \\
        \mathbb{V}\left[\hat{e}(2)\right]  &= \sigma^{2} \left( 1 + \phi_{1} \right)\\
    \end{aligned}
\end{equation*}
And the 3-step ahead forecast as:
\begin{equation*}
    \begin{aligned}
        Z_{t+3} &= \phi_{1} Z_{t+2} + \phi_{2} Z_{t+1} + e_{t+3}
        \hat{Z}_{t+3} &= \mathbb{E}\left[Z_{t+3} | I_{t}\right] \\
                      &= \phi_{1} \hat{Z}_{t+2} + \phi_{2} \hat{Z}_{t+1} + 0 \\
        \hat{e}(3) &= Z_{t+3} - \hat{Z}_{t+3} \\
                   &= \phi_{1} \hat{e}(2) + \phi_{2} \hat{e}(1) + e_{t+3} \\
        \mathbb{V}\left[\hat{e}(3)\right]  &= \phi_{1}^{2} \sigma^{2}(1 +
        \phi_{1}^{2}) + \phi_{2}^{2}\sigma^{2} + \sigma^{2}\\
                                           &= \sigma^{2} \left( 1 +
                                           \phi_{1}^{2} + \phi_{1}^{4} +
                                       \phi_{2}^{2} \right) \\
    \end{aligned}
\end{equation*}
So (intuitively) as we try to forecast further into the future, the variance
increases.

\subsection{Forecasting with a AR(p) process}
The model looks like:
\begin{equation*}
    \begin{aligned}
        Z_{t} &= \phi_{1} Z_{t-1} + \phi_{2}Z_{t-2} + \dots + \phi_{p} Z_{t-p} +
        e_{t} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}\\
              &= e_{t} + \sum_{i=1}^{p} \phi_{i}Z_{t-i} \\
    \end{aligned}
\end{equation*}

And the forecast value will look like:
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+1} &= \mathbb{E}\left[Z_{t+1} | I_{t}\right]  \\
                      &= \mathbb{E}\left[ e_{t+1} + \sum_{i=1}^{p} \phi_{i}Z_{t-i+1} | I_{t} \right]  \\
                      &= \sum_{i=1}^{p} \phi_{i}Z_{t-i+1} \\
    \end{aligned}
\end{equation*}
This equation is given without much explanation really.
\begin{equation*}
    \begin{aligned}
        Z_{t+l} = e_{t+l} + \sum_{i=1}^{p} \phi_{i} Z_{t + l - i}
        \hat{Z}_{t+l} = \sum_{i=1}^{p} \phi_{i} Z_{t + l - i}
    \end{aligned}
\end{equation*}

\subsection{Stationarity and Invertibility}
Consider the AR process:
\begin{equation*}
    Z_{t} = \phi Z_{t-1} + e_{t} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}

If $|\phi| < 1$ then we can rewrite the model with the backshift operator like:
\begin{equation*}
    \begin{aligned}
        Z_{t} &= \phi B Z_{t} + e_{t} \\
        \explain{(1 - \phi B)Z_{t}}{The characteristic polynomial} \\
    \end{aligned}
\end{equation*}
And so we can get the \textbf{Characteristic equation} by substituting $B =
\frac{1}{x}$ :
\begin{equation*}
    \begin{aligned}
        0 &= 1 - \phi B \\
        1 - \frac{\phi}{x} &= 0 \\
        x - \phi &= 0 \\
    \end{aligned}
\end{equation*}
So our root is $x = \phi $. If this root lies inside the unit circle in the
complex plane, then the time series model is stationary.

\textbf{Invertible}: we can take the process and rewrite it as an infinite
moving average process.

In this case, our AR process is invertible. Let's write it as an infinite
moving average process:
\begin{equation*}
    \begin{aligned}
        (1 - \phi B) Z_{t} &= e_{t} \\
        Z_{t} &= \explain{\frac{e_{t}}{1 - \phi B}}{Note $|\phi| < 1$} \\
        Z_{t} &= \sum_{i=0}^{\infty} \left( \phi B \right)^{i} e_{t} \\
        Z_{t} &= \sum_{i=0}^{\infty} \phi^{i} B^{i} e_{t} \\
        Z_{t} &= \sum_{i=0}^{\infty} \phi^{i} e_{t-i} \\
    \end{aligned}
\end{equation*}

So to check invertibility:
\begin{enumerate}
    \item Get the characteristic roots
    \item Check they lie inside the unit circle 
    \item  Therefore they are stationary
    \item Then check if you can write the process as an infinite moving average
        process.
\end{enumerate}

\section{The Autoregressive Moving Average Process}
\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=d6af6a2d-e789-4b3d-8c7a-c1a4c91200f8}{Time Series Video 24}}

\subsection{Intro to ARMA(p,q)}

The ARMA process has both Autoregressive components and moving average
components. And ARMA(p,q) process is shown below, where $p$ is the order of the
order of the autoregressive component. $q$ is the order of the moving average
component:
\begin{equation*}
    \begin{aligned}
        Z_{t} &= \phi Z_{t-p} + e_{t} - \theta e_{t-q} \qquad \text{Where $e_t \iid N(0, \sigma^2)$} \\
    \end{aligned}
\end{equation*}
Assuming the model is weakly stationary of order 2, we find the various
properties by:
\begin{equation*}
    \begin{aligned}
        Z_{t} Z_{t-1} &= \phi Z_{t-1}^{2} + e_{t}Z_{t-1} - \theta
        e_{t-1}Z_{t-1}\\
        \mathbb{E}\left[Z_{t} Z_{t-1}\right] &= \gamma_{1} = \phi \gamma_{0} +
        0 - \theta \sigma^{2}\\
            &= \phi\gamma_{0} - \theta \sigma^{2} \\
        \gamma_{2} &= \text{calculated in a similar way to $\gamma_{1}$} \\
        \mathbb{V}\left[Z_{t}\right] &= \mathbb{E}\left[Z_{t}^{2}\right] \\
            &= \mathbb{E}\left[\phi^{2}Z_{t-1}^{2} + e_{t}^{2} +
            \theta^{2}e_{t-1}^{2} + 2\phi Z_{t-1} e_{t} - 2 \theta \phi
        e_{t-1}Z_{t-1} - 2 \theta e_{t} e_{t-1}\right]  \\
            &= \phi^{2} \explain{\mathbb{E}\left[Z_{t}^{2}\right]}{due to
        WS(2)} + \sigma^{2} + \theta^{2}\sigma^{2} - 2 \theta \phi \sigma^{2} \\
            \mathbb{V}\left[Z_{t}\right] - \phi^{2}
        \mathbb{V}\left[Z_{t}\right] &=  \sigma^{2} + \theta^{2}\sigma^{2} -
        2\theta \phi \sigma^{2} \\
        \mathbb{V}\left[Z_{t}\right] = \frac{\sigma^{2}(1 + \theta^{2} -
        2\theta \phi)}{1 - \phi^{2}}\\
    \end{aligned}
\end{equation*}
Exact expressions for variance, autocovariance, and autocorrelation are left as
an exercise for the reader.

\subsection{Identifying the order of an ARMA process}
It's not super simple to identify the order(s) of an ARMA process. In general
we tentatively find the parameters of the ARMA process, often splitting it up
into the AR and MA processes and then solving them separately. Finally we'd
join them together and iteratively evaluate our proposed model against a
(possibly better) alternative model.

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=68df0118-3dea-4a29-a9d9-d98f45f554b7}{Time Series Video 25}}
When identifying the order(s) of an ARMA process, we often want to try fit the
data to an AR process and MA process separately, and then use those values as a
starting point for fitting the ARMA process.

\subsection{Forecasting with an ARMA Process}
With the first order ARMA process like:
\begin{equation*}
    Z_{t} = \phi Z_{t-1} + e_{t} - \theta e_{t-1} \qquad \text{Where $e_t \iid N(0, \sigma^2)$}
\end{equation*}
So the 1-step forecast can be found like:
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+1} &= \mathbb{E}\left[Z_{t+1} | I_{t}\right]  \\
                      &= \mathbb{E}\left[\phi Z_{t} + e_{t+1} - \theta e_{t} | I_{t}\right]  \\
                      &= \phi Z_{t} + 0 - \theta e_{t} \\
        \hat{e}(1) &= Z_{t+1} - \hat{Z}_{t+1} = e_{t+1} \\
        \mathbb{V}\left[ \hat{e}(1)\right] &= \sigma^{2}\\
    \end{aligned}
\end{equation*}
And the 1-step forecast can be found like:
\begin{equation*}
    \begin{aligned}
        \hat{Z}_{t+2} &= \mathbb{E}\left[Z_{t+2} | I_{t}\right]  \\
                      &= \mathbb{E}\left[\phi Z_{t+1} + e_{t+2} - \theta e_{t+1} | I_{t}\right]  \\
                      &= \phi \mathbb{E}\left[Z_{t+1} | I_{t}\right]  + \mathbb{E}\left[e_{t+2} | I_{t}\right]  - \theta \mathbb{E}\left[e_{t+1} | I_{t}\right]  \\
                      &= \phi \hat{Z}_{t+1} + 0 + 0 \\
        \hat{e}(2) &= Z_{t+2} - \hat{Z}_{t+2} = \phi \hat{e}(1) + e_{t+2} - \theta e_{t+1}\\
        \mathbb{V}\left[ \hat{e}(2)\right] &= \phi^{2}\sigma^{2} + \sigma^{2} + \theta^{2}\sigma^{2}\\
        &= \sigma^{2} \left( \phi^{2} + 1 + \theta^{2}\right) \\
    \end{aligned}
\end{equation*}
Note that it can be shown that as we forecast into the future, the forecast
value converges onto the expectation of the time series:
\begin{equation*}
    \lim_{l \to \infty} \hat{Z}_{t+l} = \mathbb{E}\left[Z_{t}\right] 
\end{equation*}
And there's a similar result for the variance of the error:
\begin{equation*}
    \lim_{l \to \infty} \mathbb{V}\left[ \hat{e}(l)\right] = \mathbb{V}\left[ Z_{t} \right] 
\end{equation*}


\section{The ARIMA Process}
This is the first of our models which is non-stationary. It stands for an
Autoregressive Integrated Moving Average Process. Our overall tactic here will
be to transform ARIMA processes into stationary time series so that we can
analyse it more easily. It is parametrised by ARIMA(p, d, q) where the $d$ is
degree of integration.

\marginpar{\href{https://media.uct.ac.za/engage/theodul/ui/core.html?ltimode=true&id=87145e3d-c014-42e8-ad6a-46b293c190fe}{Time Series Video 26}}
In practice, many time series models are non-stationary. The ARIMA process is
denoted by
\subsection{Difference Operator}
Define the $\nabla$ operator as:
\begin{equation*}
    \begin{aligned}
        \nabla Z_{t} &:= Z_{t} - Z_{t-1}\\
        \nabla^{n} Z_{t} &:= \nabla^{n-1}\left(\nabla Z_{t}\right)\\
        \nabla^{2} Z_{t} &= Z_{t} - 2Z_{t-1} + Z_{t-2} \\
    \end{aligned}
\end{equation*}
\subsection{Making a ARIMA process stationary}
We'll be using the $\nabla $ operator to convert a non-stationary time series
into a stationary time series.
If the time series $W_{t}$ given by:
\begin{equation*}
    W_{t} = \nabla Z_{t} = Z_{t} - Z_{t-1}
\end{equation*}
is stationary, then $Z_{t}$ is from an ARIMA(p, 1, q) process.

\textbf{Or in general}, if you have to use the $\nabla $ difference operator
$d$ times in order to convert the non-stationary process $Z_{t}$ into a
stationary process $W_{t}$, then we say $W_{t} \sim \text{ARIMA}(p, d, q)$.

Note that sometimes the MA or AR component will fall away in the process of
taking the difference:
\begin{itemize}
    \item If the differenced, now stationary process $W_{t}$ only has an MA
        component (and no AR component) then we call $Z_{t}$ an IMA(0,d,q)
        process, Integrated Moving Average process.
    \item If the differenced, now stationary process $W_{t}$ only has an AR
        component (and no MA component) then we call $Z_{t}$ an IAR(p,d,0)
        process, Integrated Autoregressive process.
\end{itemize}
\subsection{Finding the order of an ARIMA Process}
The ARIMA process has three orders: p, d, q.  We can look at the
autocorrelation function (ACF) and partial autocorrelation function (PACT), and
if the time series is stationary then a graph of lag vs correlation will show
that as the lag increases, the autocorrelation goes to zero.

\begin{itemize}
    \item \textbf{Stationary Series}: If the autocorrelations go to zero as the
        lag increases, then the series is probably stationary.
    \item \textbf{Bartlett's test}: $\frac{2}{\sqrt{n}}$ can tell you how many
        autocorrelations are statistically significant.
\end{itemize}

The above will determine if the current time series is stationary or not. If it
is stationary, then all is good and you know that the order of d is 1. If it's
not stationary, then you need to use the difference operator $\nabla Z_{t}$ to
get a new time series, and then perform the same tests on this differenced time
series. The number of times you have to use the difference $\nabla $ operator
is the order d of the data.


Once we've got a stationary process, then we can try find p and q by treating
it as an ARMA(p,q) model and examining it as in the ARMA section above.

\section{Box-Jenkins Algorithm}

This is a high level method for processing a given time series $Z_{t}$ and
attempting to build a model for it.
\begin{enumerate}
    \item If $Z_{t}$ is not stationary, difference $\nabla $ it until you've
        got a stationary process.
    \item Examine the ACF and PACF of the (now stationary) time series using
        the Bartlett's test to test for statistical significance.
    \item From the number of statistically significant ACFs and PACFs,
        tentatively model the data with the ARMA(p,q) process. $p$ is the
        number of statistically significant ACFs, $q$ is the number of
        statistically significant PACFs, 
    \item Evaluate the tentative model with Ordinary Least Squares, Method
        of Maximum Likelihood, or Method of Moments estimates.
    \item Then check for \textbf{serial autocorrelation} to make sure that the
        error terms actually are independent and identically distributed.
    \item If the errors aren't iid, then we need to adjust the parameters for p
        and q until they are.
    \item Also check that the parameters of the model ($\phi_{i}, \theta_{i}, $
        etc) are statistically significant with the usual t-tests.
    \item Now that the errors are iid, we can use the model to forecast.
\end{enumerate}


Note that differencing too many times can lead to unwanted correlations.  For
instance, for the model $Z_{t} = Z_{t-1} + e_{t}$ then the difference is given
by:
\begin{equation*}
    \begin{aligned}
        \nabla Z_{t} &= Z_{t} - Z_{t-1} = e_{t} \\
        &\text{This is a stationary series, and we should stop here.}\\
        &\text{But if we continue:}\\
        \nabla (\nabla Z_{t}) &= e_{t} - e_{t-1}\\
        &\text{we get a MA process looking like:}\\
        Z_{t} &= e_{t} + \theta e_{t-1} \\
    \end{aligned}
\end{equation*}
So over differencing can erroneously introduce parameters (like $\theta $
above) which aren't actually part of the underlying model.


