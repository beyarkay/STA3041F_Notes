\section{3041F: Chains}
\subsection{Chapman-Kolmogorov Equations}
Basically, for Markov chains we can just do matrix multiplication to go from time \(i\) to time \(i + 1\).
For \(0 \le v \le t\), and \(p_{ij}(s, t) := \mathbb{P}[X_{s+t} = j | X_s = i]\):
\begin{equation*}
    p_{ij}(s, t) = \mathbb{P}[X_{s+t} = j | X_s = i] = 
    \sum_{k \in \mathcal{U}}p_{ik}(s, v) \cdot p_{kj}(s + v, t - v)
\end{equation*}

Or, in matrix notation:

\begin{equation*}
    \mathbf{P}^{(t)}(s) = \mathbf{P}^{(v)}(s) \cdot \mathbf{P}^{(t-v)}(s + v)
\end{equation*}
And under time homogeneity
\begin{equation*}
    \mathbf{P}^{(t)} = \mathbf{P}^{v} \cdot \mathbf{P}^{t-v} = \mathbf{P}^{t} 
\end{equation*}
\section{3041F: Branching Processes}
Let \(Z_m\) be the number of individuals in Generation \(m\).
Let \(X_{im}\) be the number of offspring produced by individual \(i\) in generation \(m\).
Assume all \(X_{im} = X\), so all \(X_{im}\) are identically distributed.
Assume \(Z_0 = 1\) which implies \(Z_1 = X\).

\begin{equation*}
    \mathbb{E}[Z_m] = \mathbb{E}[X]^m
\end{equation*}

Let \(\mu =\mathbb{E}[X]\) and \(\sigma^2 = \mathbb{V}[X]\).

\begin{equation*}
    \mathbb{V}[Z_m] = m\sigma^2 \qquad \text{if} \, \mu = 1
\end{equation*}
\begin{equation*}
    \mathbb{V}[Z_m] = \sigma^2 \cdot \mu^{m-1} \left( \frac{\mu^{m} - 1}{\mu - 1} \right) \qquad \text{if}\, \mu \ne 1
\end{equation*}
\section{3041F: Probability of Extinction}
To calculate the probability that the process is extinct by generation m:
\begin{equation*}
    \mathbb{P}[\text{Extinction by generation } m] = \mathbb{P}[Z_m = 0] = \mathbb{G}_m(0)
\end{equation*}

But to calculate the probability that the process is extinct at exactly generation m:
\begin{equation*}
    \mathbb{P}[\text{Extinction at generation } m] = \mathbb{G}_m(0) - \mathbb{G}_{m-1}(0)
\end{equation*}

Define the probability of extinction:
\begin{equation*}
    \begin{aligned}
        \eta :&= \mathbb{P}[\text{Eventual Extinction}] = \mathbb{P}\left[\bigcup_{m=1}^{\infty} \{ Z_m = 0 \}\right] \\
              &= \text{smallest non-negative integer root of } \mathbb{G}_X(s) = s \\
    \end{aligned}
\end{equation*}
\section{3041F: First Passage Probabilities}
\begin{equation*}
    \begin{aligned}
        p_{ij}^{(n)} :&= \mathbb{P}[X_{t+n} = j | X_{t} = i]  \\
        P &= \left[ p_{ij}\right]\\
        P_{ij}(s) :&= \sum_{n=0}^{\infty} p_{ij}^{(n)} \cdot s^n \\
        \mathbf{P}(s) :&= \left[ P_{ij}(s) \right]\\
                       &= \left( \mathbf{I} - s \cdot P \right)^{-1}\\
                       &= \left( \mathbf{I} - s \cdot \left[ p_{ij}\right] \right)^{-1}\\
        f_{ij}^{(k)} &= \mathbb{P}[\text{a k-step passage from i to j}] \qquad f_{ij}^{(0)} = 0 \,\forall\, i, j \\
        F_{ij}(s) &= \sum_{n=0}^{\infty} f_{ij}^{(n)} \cdot s^n \\
    \end{aligned}
\end{equation*}

Don't forget the test comes with identities for converting Probability matrices to first return probabilities.
\section{3041F: Discrete Time}
\subsection{Calculating the Count at a given Time}
\subsubsection{Forecasting}
\begin{equation}
    \mathbb{P}[N_{n+k} = a + i | N_n = a] = 
    \binom{k}{i} \cdot p^i \cdot q^{k-i}
\end{equation}

\begin{equation}
    (N_{n+k} - N_n) \sim Binomial(k, p)
\end{equation}

\begin{equation}
    (N_{n+k} - N_n) \indep N_n
\end{equation}

\begin{equation}
    \mathbb{E}[N_{n+k} | N_n = a] = k \cdot p + a
\end{equation}

\begin{equation}
    \mathbb{V}[N_{n+k} | N_n = a] = k \cdot p \cdot a
\end{equation}

\subsubsection{Backcasting}
\begin{equation}
    \mathbb{P}[N_n = a | N_{n+k} = a + i] = 
    \frac{
        \binom{n}{a} \binom{k}{i}
        }{
        \binom{n+k}{a+i}
    }
\end{equation}
\begin{equation}
    \mathbb{P}[N_n = a | N_{n+k} = a + i] \sim HyperGeo(n+k, n, a+i)
\end{equation}

\begin{equation}
    \mathbb{E}[N_n | N_{n+k} = a + i] = \frac{(a + i) \cdot n}{n + k}
\end{equation}

\begin{equation}
    \mathbb{V}[N_n | N_{n+k} = a + i] = 
    \frac{n + k - a - i}{n + k - 1} \cdot 
    \frac{(a + i) \cdot n}{ n + k} \cdot 
    ( 1 - \frac{n}{n + k})
\end{equation}

\subsection{Calculating the Time until a given Count}
\begin{equation}
    T_a \sim NegBinomial(a, p) 
\end{equation}

\begin{equation}
    \mathbb{E}[T_a] = \frac{a}{p}
\end{equation}

\begin{equation}
    \mathbb{V}[T_a] = \frac{a \cdot q}{p^2}
\end{equation}


\subsubsection{Forecasting}
\begin{equation}
    \mathbb{P}[T_{a + i} = t + j | T_a = t] =
    \binom{j-1}{i-1} \cdot p^i \cdot q^{j-i}
\end{equation}

\begin{equation}
    \mathbb{P}[T_{a + i} = t + j | T_a = t] \sim NegBinomial(i, p)
\end{equation}


\begin{equation}
    \mathbb{P}[T_{a + i} | T_a = t] = 
    \frac{i}{p} + t
\end{equation}

\begin{equation}
    \mathbb{P}[T_{a + i} | T_a = t] = 
    \frac{i \cdot q}{p^2}
\end{equation}

\subsubsection{Backcasting}
\begin{equation}
    \mathbb{P}[T_a = t | T_{a + i} = t + j] =
    \frac{
        \binom{t - 1}{a - 1} \cdot \binom{j - 1}{i - 1}
        }{
        \binom{t + j - 1}{a + i - 1}
    }
\end{equation}

\begin{equation}
    \mathbb{E}[T_a | T_{a + i} = t + j] = \frac{a (t + j)}{ a + i }
\end{equation}


\begin{equation}
    \mathbb{V}[T_a | T_{a + i} = t + j] = 
    \frac{
        a i (t + j) (t + j - a - i)
        }{
        (a + i)^2 (a + i + 1)
    }
\end{equation}
\section{3041F: Continuous Time}
With the assumptions that \(\lambda_i = \lambda\) and \(t_i = t\) for all i:

\begin{equation}
    X_i \sim Poisson(\lambda t)
\end{equation}

\begin{equation}
    Y_n = \sum_{i = 1}^{n} X_i
\end{equation}

\begin{equation}
    Y_n \sim Poisson(n \lambda t)
\end{equation}

\begin{equation}
    Y_{n+k} - Y_n \sim Poisson(k \lambda t)
\end{equation}

\begin{equation}
    \mathbb{C}ov[Y_n, Y_m] = \lambda \cdot t \cdot min(n, m)
\end{equation}

\begin{equation}
    \mathbb{C}orr[Y_n, Y_m] = \frac{min(n, m)}{\sqrt{nm}}
\end{equation}

\subsection{Calculating the Count at a given Time}
\subsubsection{Forecasting}
\begin{equation}
    \mathbb{P}[Y_{n + k} = a + i | Y_n = a] =
    \frac{
        e^{-k \lambda t} \cdot (k \lambda t)^i
    }{ i! }
\end{equation}

\begin{equation}
    \mathbb{E}[Y_{n + k} | Y_n = a] = k \lambda t + a
\end{equation}

\begin{equation}
    \mathbb{V}[Y_{n + k} | Y_n = a] = k \lambda t
\end{equation}

\subsubsection{Backcasting}
\begin{equation}
    \mathbb{P}[Y_n = a | Y_{n + k} = a + i ] =
    \binom{a + i}{a} 
    \left(\frac{n}{n + k}\right)^a 
    \left(\frac{k}{n + k}\right)^i
\end{equation}

\begin{equation}
    \mathbb{P}[Y_n = a | Y_{n + k} = a + i ] 
    \sim Binomial\left(a + i, \frac{n}{n+k}\right)
\end{equation}

\begin{equation}
    \mathbb{E}[Y_n | Y_{n + k} = a + i ] =
    \frac{(a + i) \cdot n}{n+k}
\end{equation}

\begin{equation}
    \mathbb{V}[Y_n | Y_{n + k} = a + i ] = 
    \frac{(a + i) \cdot n \cdot k}{(n + k)^2}
\end{equation}

\subsection{Calculating the Time until a given Count}

\begin{equation}
    T_1 \sim Exponential\left(\frac{1}{\lambda}\right)
\end{equation}

\begin{equation}
    T_a \sim Gamma(a, \lambda)
\end{equation}

\begin{equation}
    \mathbb{E}[T_a] = \frac{a}{\lambda}
\end{equation}

\begin{equation}
    \mathbb{V}[T_a] = \frac{a}{\lambda^2}
\end{equation}

\subsubsection{Forecasting}
\begin{equation}
    \mathbb{E}[T_{a + i} | T_a = t] = \frac{i}{\lambda} + t
\end{equation}

\begin{equation}
    \mathbb{V}[T_{a + i} | T_a = t] = \frac{i}{\lambda^2}
\end{equation}


\subsubsection{Backcasting}
\begin{equation}
    \mathbb{E}[T_a | T_{a + i} = t + j] = \frac{a(t + j)}{a + i}
\end{equation}

\begin{equation}
    \mathbb{V}[T_a | T_{a + i} = t + j] = 
    \frac{ ai(t + j)^2 }{ (a + i)^2 \cdot (a + i + 1) }
\end{equation}
